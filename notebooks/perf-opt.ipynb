{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c446308-24de-4981-8f6a-7d2ebbfb3cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "from ddpm import DDPM, ModelConfig, TrainerConfig\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1461c4bc-39ac-4c62-b9d5-4e31669fc8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d760c41d-f894-4f8c-82a2-d3ac58b4d302",
   "metadata": {},
   "source": [
    "[A100 Spec](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a59c92d-a47f-4545-9c5a-07eddc028965",
   "metadata": {},
   "outputs": [],
   "source": [
    "A100_FP32_TFLOPS_PER_SEC = 19.5\n",
    "A100_FP16_TFLOPS_PER_SEC = 312\n",
    "A100_MEMBW_TB_PER_SEC = 1.555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e67ec70-7002-4afc-9ac3-996cb5ec32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_TFLOPS_PER_SEC = 2.91"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e56d46-b2ba-4943-adce-45ff3b1a6de0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Baseline\n",
    "\n",
    "2.91 TFLOP/s per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "790f221b-d5a9-42d8-a638-02f7ad76aec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "img2tensor = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=0.5, std=0.5)  # [0, 1] -> [-1, 1]\n",
    "])\n",
    "ds = CIFAR10('./cifar10', train=True, transform=img2tensor, download=True)\n",
    "cfg_m = ModelConfig()\n",
    "cfg_t = TrainerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c92c67be-5b78-43dc-9c91-df33f9ed0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ds, batch_size=cfg_t.bs, num_workers=cfg_t.num_workers, drop_last=True)\n",
    "ddpm = DDPM(**asdict(cfg_m)).to('cuda')\n",
    "optimizer = torch.optim.AdamW(ddpm.parameters(), lr=cfg_t.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe9be17b-208b-4b2f-b493-64dd0d31b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, _ = next(iter(dataloader))\n",
    "x0 = x0.to('cuda')\n",
    "eps = torch.randn_like(x0)\n",
    "t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f5771-1222-4f70-bf4f-233f03ca5263",
   "metadata": {},
   "source": [
    "Using `torchinfo` to estimate the FLOPs. ([`torchinfo` docs](https://pypi.org/project/torchinfo/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4291b4b3-0b82-4a4c-804c-2cef1734089c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "DDPM                                          [128, 3, 32, 32]          --\n",
       "├─UNet: 1-1                                   [128, 3, 32, 32]          --\n",
       "│    └─Sequential: 2-1                        [128, 128]                --\n",
       "│    │    └─Linear: 3-1                       [128, 128]                4,224\n",
       "│    │    └─GELU: 3-2                         [128, 128]                --\n",
       "│    │    └─Linear: 3-3                       [128, 128]                16,512\n",
       "│    └─Conv2d: 2-2                            [128, 32, 32, 32]         128\n",
       "│    └─UNetDownsample: 2-3                    [128, 32, 16, 16]         --\n",
       "│    │    └─TimeResNetBlock: 3-4              [128, 32, 32, 32]         26,880\n",
       "│    │    └─TimeResNetBlock: 3-5              [128, 32, 32, 32]         26,880\n",
       "│    │    └─GroupNorm: 3-6                    [128, 32, 32, 32]         64\n",
       "│    │    └─Attention: 3-7                    [128, 32, 32, 32]         16,416\n",
       "│    │    └─DownsampleOutProject: 3-8         [128, 32, 16, 16]         4,128\n",
       "│    └─UNetDownsample: 2-4                    [128, 64, 8, 8]           --\n",
       "│    │    └─TimeResNetBlock: 3-9              [128, 32, 16, 16]         26,880\n",
       "│    │    └─TimeResNetBlock: 3-10             [128, 32, 16, 16]         26,880\n",
       "│    │    └─GroupNorm: 3-11                   [128, 32, 16, 16]         64\n",
       "│    │    └─Attention: 3-12                   [128, 32, 16, 16]         16,416\n",
       "│    │    └─DownsampleOutProject: 3-13        [128, 64, 8, 8]           8,256\n",
       "│    └─UNetDownsample: 2-5                    [128, 128, 4, 4]          --\n",
       "│    │    └─TimeResNetBlock: 3-14             [128, 64, 8, 8]           90,624\n",
       "│    │    └─TimeResNetBlock: 3-15             [128, 64, 8, 8]           90,624\n",
       "│    │    └─GroupNorm: 3-16                   [128, 64, 8, 8]           128\n",
       "│    │    └─Attention: 3-17                   [128, 64, 8, 8]           32,832\n",
       "│    │    └─DownsampleOutProject: 3-18        [128, 128, 4, 4]          32,896\n",
       "│    └─UNetDownsample: 2-6                    [128, 256, 4, 4]          --\n",
       "│    │    └─TimeResNetBlock: 3-19             [128, 128, 4, 4]          328,704\n",
       "│    │    └─TimeResNetBlock: 3-20             [128, 128, 4, 4]          328,704\n",
       "│    │    └─GroupNorm: 3-21                   [128, 128, 4, 4]          256\n",
       "│    │    └─Attention: 3-22                   [128, 128, 4, 4]          65,664\n",
       "│    │    └─Conv2d: 3-23                      [128, 256, 4, 4]          295,168\n",
       "│    └─UNetBlock: 2-7                         [128, 256, 4, 4]          --\n",
       "│    │    └─TimeResNetBlock: 3-24             [128, 256, 4, 4]          1,247,232\n",
       "│    │    └─TimeResNetBlock: 3-25             [128, 256, 4, 4]          1,247,232\n",
       "│    │    └─GroupNorm: 3-26                   [128, 256, 4, 4]          512\n",
       "│    │    └─Attention: 3-27                   [128, 256, 4, 4]          131,328\n",
       "│    └─UNetUpsample: 2-8                      [128, 128, 8, 8]          --\n",
       "│    │    └─TimeResNetBlock: 3-28             [128, 256, 4, 4]          1,640,704\n",
       "│    │    └─TimeResNetBlock: 3-29             [128, 256, 4, 4]          1,640,704\n",
       "│    │    └─GroupNorm: 3-30                   [128, 256, 4, 4]          512\n",
       "│    │    └─Attention: 3-31                   [128, 256, 4, 4]          131,328\n",
       "│    │    └─Sequential: 3-32                  [128, 128, 8, 8]          295,040\n",
       "│    └─UNetUpsample: 2-9                      [128, 64, 16, 16]         --\n",
       "│    │    └─TimeResNetBlock: 3-33             [128, 128, 8, 8]          427,136\n",
       "│    │    └─TimeResNetBlock: 3-34             [128, 128, 8, 8]          427,136\n",
       "│    │    └─GroupNorm: 3-35                   [128, 128, 8, 8]          256\n",
       "│    │    └─Attention: 3-36                   [128, 128, 8, 8]          65,664\n",
       "│    │    └─Sequential: 3-37                  [128, 64, 16, 16]         73,792\n",
       "│    └─UNetUpsample: 2-10                     [128, 32, 32, 32]         --\n",
       "│    │    └─TimeResNetBlock: 3-38             [128, 64, 16, 16]         115,264\n",
       "│    │    └─TimeResNetBlock: 3-39             [128, 64, 16, 16]         115,264\n",
       "│    │    └─GroupNorm: 3-40                   [128, 64, 16, 16]         128\n",
       "│    │    └─Attention: 3-41                   [128, 64, 16, 16]         32,832\n",
       "│    │    └─Sequential: 3-42                  [128, 32, 32, 32]         18,464\n",
       "│    └─UNetUpsample: 2-11                     [128, 32, 32, 32]         --\n",
       "│    │    └─TimeResNetBlock: 3-43             [128, 32, 32, 32]         38,176\n",
       "│    │    └─TimeResNetBlock: 3-44             [128, 32, 32, 32]         38,176\n",
       "│    │    └─GroupNorm: 3-45                   [128, 32, 32, 32]         64\n",
       "│    │    └─Attention: 3-46                   [128, 32, 32, 32]         16,416\n",
       "│    │    └─Conv2d: 3-47                      [128, 32, 32, 32]         9,248\n",
       "│    └─TimeResNetBlock: 2-12                  [128, 32, 32, 32]         --\n",
       "│    │    └─Linear: 3-48                      [128, 64]                 8,256\n",
       "│    │    └─Sequential: 3-49                  [128, 32, 32, 32]         18,528\n",
       "│    │    └─Sequential: 3-50                  [128, 32, 32, 32]         9,312\n",
       "│    │    └─Conv2d: 3-51                      [128, 32, 32, 32]         2,080\n",
       "│    └─Conv2d: 2-13                           [128, 3, 32, 32]          99\n",
       "===============================================================================================\n",
       "Total params: 9,190,211\n",
       "Trainable params: 9,190,211\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 61.22\n",
       "===============================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 2675.11\n",
       "Params size (MB): 36.76\n",
       "Estimated Total Size (MB): 2715.02\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(ddpm, input_data=[x0, eps, t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919bda1-2575-45d4-b4c0-cb92e2c57e5a",
   "metadata": {},
   "source": [
    "Forward pass FLOPs $= 61.22 \\times 2 = 132.44 \\implies$ forward + backward pass $= 61.22 \\times 3 = 183.66$ GFLOPs  \n",
    "[Forward and Backward Pass FLOP estimation](https://epochai.org/blog/backward-forward-FLOP-ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ff186-7b02-494a-a508-5b7766f99e93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Profiling\n",
    "\n",
    "- [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)\n",
    "- [Using CUDA events to measure time](https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d8750b-0ed5-4a26-8b5e-0f83c9bfc517",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PyTorch Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ffd20b2-fb80-48f9-b220-a853fadab5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function\n",
    "from torch.profiler import ProfilerActivity as PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16081097-d0b1-4466-a528-02b17cf4bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-06-18 07:27:37 1493321:1493321 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-06-18 07:27:38 1493321:1493321 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-06-18 07:27:38 1493321:1493321 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "ddpm.train()\n",
    "\n",
    "with profile(activities=[PA.CPU, PA.CUDA], record_shapes=True) as prof:\n",
    "    for x0, _ in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x0 = x0.to('cuda')\n",
    "        eps = torch.randn_like(x0)\n",
    "        t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "        \n",
    "        eps_pred = ddpm(x0, eps, t)\n",
    "        loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bacdfbf-6062-4a1e-9af7-55326c9646f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.62%     855.000us         9.27%      12.747ms     169.960us       0.000us         0.00%      17.319ms     230.920us            75  \n",
      "                                   ConvolutionBackward0         0.30%     406.000us         8.49%      11.674ms     155.653us       0.000us         0.00%      16.942ms     225.893us            75  \n",
      "                             aten::convolution_backward         4.51%       6.208ms         8.19%      11.268ms     150.240us      15.426ms        28.86%      16.942ms     225.893us            75  \n",
      "                                              aten::bmm         0.75%       1.037ms         1.04%       1.434ms      26.556us      10.142ms        18.97%      10.142ms     187.815us            54  \n",
      "                                      aten::convolution         0.69%     948.000us         5.52%       7.593ms     101.240us       0.000us         0.00%       6.295ms      83.933us            75  \n",
      "                                     aten::_convolution         0.53%     733.000us         4.83%       6.645ms      88.600us       0.000us         0.00%       6.295ms      83.933us            75  \n",
      "                                           aten::conv2d         0.28%     384.000us         5.67%       7.794ms     103.920us       0.000us         0.00%       6.198ms      82.640us            75  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.09%     121.000us         0.90%       1.241ms      68.944us       0.000us         0.00%       6.088ms     338.222us            18  \n",
      "                                           BmmBackward0         0.10%     135.000us         0.80%       1.104ms      61.333us       0.000us         0.00%       5.980ms     332.222us            18  \n",
      "                                aten::cudnn_convolution         2.51%       3.454ms         3.40%       4.675ms      62.333us       5.434ms        10.17%       5.434ms      72.453us            75  \n",
      "                                           aten::matmul         0.14%     188.000us         1.45%       1.994ms     110.778us       0.000us         0.00%       5.111ms     283.944us            18  \n",
      "autograd::engine::evaluate_function: NativeGroupNorm...         0.40%     549.000us         4.45%       6.117ms     130.149us       0.000us         0.00%       4.033ms      85.809us            47  \n",
      "_ZN19cutlass_cudnn_train6KernelINS_4conv6kernel23Imp...         0.00%       0.000us         0.00%       0.000us       0.000us       3.932ms         7.36%       3.932ms     302.462us            13  \n",
      "                               NativeGroupNormBackward0         0.26%     355.000us         3.97%       5.465ms     116.277us       0.000us         0.00%       3.921ms      83.426us            47  \n",
      "                       aten::native_group_norm_backward         1.66%       2.284ms         3.60%       4.954ms     105.404us       3.873ms         7.25%       3.873ms      82.404us            47  \n",
      "                                ampere_sgemm_128x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us       3.835ms         7.17%       3.835ms     639.167us             6  \n",
      "                                ampere_sgemm_128x128_tt         0.00%       0.000us         0.00%       0.000us       0.000us       3.428ms         6.41%       3.428ms     571.333us             6  \n",
      "                                              aten::sum         1.88%       2.591ms         2.71%       3.723ms      18.522us       2.829ms         5.29%       2.829ms      14.075us           201  \n",
      "void cudnn::ops::nchwToNhwcKernel<float, float, floa...         0.00%       0.000us         0.00%       0.000us       0.000us       2.664ms         4.98%       2.664ms      10.571us           252  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       2.616ms         4.89%       2.616ms      14.533us           180  \n",
      "                                            aten::copy_         0.34%     474.000us         2.85%       3.920ms      11.632us       2.464ms         4.61%       2.464ms       7.312us           337  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       2.329ms         4.36%       2.329ms      40.155us            58  \n",
      "                                          aten::reshape         0.64%     875.000us         1.94%       2.670ms       9.745us       0.000us         0.00%       2.138ms       7.803us           274  \n",
      "                                            aten::clone         0.24%     332.000us         1.25%       1.716ms      29.085us       0.000us         0.00%       2.114ms      35.831us            59  \n",
      "cudnn_infer_ampere_scudnn_winograd_128x128_ldg1_ldg4...         0.00%       0.000us         0.00%       0.000us       0.000us       1.951ms         3.65%       1.951ms      97.550us            20  \n",
      "                                              aten::mul         2.44%       3.360ms         3.71%       5.107ms      14.467us       1.719ms         3.22%       1.719ms       4.870us           353  \n",
      "                                       aten::group_norm         0.12%     165.000us         2.95%       4.053ms      86.234us       0.000us         0.00%       1.670ms      35.532us            47  \n",
      "                                aten::native_group_norm         1.47%       2.016ms         2.83%       3.888ms      82.723us       1.670ms         3.12%       1.670ms      35.532us            47  \n",
      "                                             aten::add_         1.35%       1.859ms         2.71%       3.731ms       7.093us       1.624ms         3.04%       1.624ms       3.087us           526  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.610ms         3.01%       1.610ms       9.415us           171  \n",
      "      autograd::engine::evaluate_function: MulBackward0         0.44%     607.000us         2.49%       3.418ms      51.788us       0.000us         0.00%       1.495ms      22.652us            66  \n",
      "sm80_xmma_wgrad_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us       1.491ms         2.79%       1.491ms     186.375us             8  \n",
      "void at::native::(anonymous namespace)::GammaBetaBac...         0.00%       0.000us         0.00%       0.000us       0.000us       1.471ms         2.75%       1.471ms      31.298us            47  \n",
      "                                              aten::cat         0.38%     521.000us         0.58%     795.000us      20.921us       1.397ms         2.61%       1.397ms      36.763us            38  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.393ms         2.61%       1.393ms      37.649us            37  \n",
      "                                             aten::mean         0.89%       1.222ms         1.28%       1.755ms      22.792us       1.275ms         2.39%       1.275ms      16.558us            77  \n",
      "                                ampere_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us       1.275ms         2.39%       1.275ms     141.667us             9  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       1.274ms         2.38%       1.274ms      16.545us            77  \n",
      "void at::native::(anonymous namespace)::ComputeInter...         0.00%       0.000us         0.00%       0.000us       0.000us       1.247ms         2.33%       1.247ms      26.532us            47  \n",
      "                              Optimizer.step#AdamW.step         4.91%       6.758ms         9.54%      13.120ms      13.120ms       0.000us         0.00%       1.226ms       1.226ms             1  \n",
      "      autograd::engine::evaluate_function: VarBackward0         0.23%     312.000us         2.23%       3.071ms      80.816us       0.000us         0.00%       1.132ms      29.789us            38  \n",
      "    autograd::engine::evaluate_function: SplitBackward0         0.11%     149.000us         0.55%     755.000us      26.964us       0.000us         0.00%       1.111ms      39.679us            28  \n",
      "                                         SplitBackward0         0.05%      71.000us         0.44%     606.000us      21.643us       0.000us         0.00%       1.111ms      39.679us            28  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.081ms         2.02%       1.081ms       7.207us           150  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_6...         0.00%       0.000us         0.00%       0.000us       0.000us       1.067ms         2.00%       1.067ms      59.278us            18  \n",
      "                                ampere_sgemm_128x128_nt         0.00%       0.000us         0.00%       0.000us       0.000us       1.065ms         1.99%       1.065ms     177.500us             6  \n",
      "                                           VarBackward0         0.27%     368.000us         1.77%       2.431ms      63.974us       0.000us         0.00%       1.030ms      27.105us            38  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.009ms         1.89%       1.009ms       6.552us           154  \n",
      "void at::native::(anonymous namespace)::RowwiseMomen...         0.00%       0.000us         0.00%       0.000us       0.000us     924.000us         1.73%     924.000us      19.660us            47  \n",
      "                                              aten::add         1.07%       1.478ms         1.56%       2.148ms      18.842us     899.000us         1.68%     899.000us       7.886us           114  \n",
      "                                              aten::var         0.40%     548.000us         0.79%       1.091ms      28.711us     886.000us         1.66%     886.000us      23.316us            38  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     886.000us         1.66%     886.000us      23.316us            38  \n",
      "void cudnn::ops::nhwcToNchwKernel<float, float, floa...         0.00%       0.000us         0.00%       0.000us       0.000us     829.000us         1.55%     829.000us       7.895us           105  \n",
      "                                           MulBackward0         0.21%     282.000us         1.43%       1.962ms      29.727us       0.000us         0.00%     777.000us      11.773us            66  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     770.000us         1.44%     770.000us      16.383us            47  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_2...         0.00%       0.000us         0.00%       0.000us       0.000us     698.000us         1.31%     698.000us      58.167us            12  \n",
      "sm80_xmma_dgrad_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     663.000us         1.24%     663.000us      51.000us            13  \n",
      "sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     639.000us         1.20%     639.000us     159.750us             4  \n",
      "autograd::engine::evaluate_function: ReshapeAliasBac...         0.13%     173.000us         0.57%     784.000us      13.517us       0.000us         0.00%     633.000us      10.914us            58  \n",
      "                                  ReshapeAliasBackward0         0.08%     110.000us         0.41%     566.000us       9.759us       0.000us         0.00%     633.000us      10.914us            58  \n",
      "_ZN19cutlass_cudnn_train6KernelINS_4conv6kernel23Imp...         0.00%       0.000us         0.00%       0.000us       0.000us     622.000us         1.16%     622.000us      62.200us            10  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.22%     306.000us         1.82%       2.501ms     119.095us       0.000us         0.00%     615.000us      29.286us            21  \n",
      "sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     606.000us         1.13%     606.000us      67.333us             9  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     600.000us         1.12%     600.000us      12.766us            47  \n",
      "autograd::engine::evaluate_function: UpsampleNearest...         0.01%      13.000us         0.06%      78.000us      26.000us       0.000us         0.00%     574.000us     191.333us             3  \n",
      "                             UpsampleNearest2DBackward0         0.01%       7.000us         0.05%      65.000us      21.667us       0.000us         0.00%     574.000us     191.333us             3  \n",
      "                      aten::upsample_nearest2d_backward         0.02%      31.000us         0.04%      58.000us      19.333us     574.000us         1.07%     574.000us     191.333us             3  \n",
      "void at::native::(anonymous namespace)::upsample_nea...         0.00%       0.000us         0.00%       0.000us       0.000us     574.000us         1.07%     574.000us     191.333us             3  \n",
      "      autograd::engine::evaluate_function: SubBackward0         0.20%     270.000us         0.98%       1.346ms      35.421us       0.000us         0.00%     517.000us      13.605us            38  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_6...         0.00%       0.000us         0.00%       0.000us       0.000us     484.000us         0.91%     484.000us      23.048us            21  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     478.000us         0.89%     478.000us       4.596us           104  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_2...         0.00%       0.000us         0.00%       0.000us       0.000us     414.000us         0.77%     414.000us      59.143us             7  \n",
      "                                         AddmmBackward0         0.14%     187.000us         1.14%       1.572ms      74.857us       0.000us         0.00%     392.000us      18.667us            21  \n",
      "                                               aten::mm         0.56%     766.000us         0.82%       1.129ms      27.537us     392.000us         0.73%     392.000us       9.561us            41  \n",
      "sm80_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     374.000us         0.70%     374.000us     187.000us             2  \n",
      "void cutlass_cudnn_infer::Kernel<cutlass_tensorop_s1...         0.00%       0.000us         0.00%       0.000us       0.000us     364.000us         0.68%     364.000us      91.000us             4  \n",
      "sm80_xmma_dgrad_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     354.000us         0.66%     354.000us      70.800us             5  \n",
      "void cutlass_cudnn_train::Kernel<cutlass_tensorop_s1...         0.00%       0.000us         0.00%       0.000us       0.000us     331.000us         0.62%     331.000us      55.167us             6  \n",
      "      autograd::engine::evaluate_function: AddBackward0         0.33%     448.000us         0.68%     932.000us       8.962us       0.000us         0.00%     322.000us       3.096us           104  \n",
      "                                              aten::sub         0.67%     924.000us         0.95%       1.300ms      17.105us     320.000us         0.60%     320.000us       4.211us            76  \n",
      "     autograd::engine::evaluate_function: SiluBackward0         0.11%     145.000us         0.38%     523.000us      27.526us       0.000us         0.00%     280.000us      14.737us            19  \n",
      "                                    aten::silu_backward         0.14%     197.000us         0.23%     317.000us      16.684us     280.000us         0.52%     280.000us      14.737us            19  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     280.000us         0.52%     280.000us      14.737us            19  \n",
      "                                    aten::_foreach_mul_         0.42%     578.000us         0.46%     638.000us     212.667us     280.000us         0.52%     280.000us      93.333us             3  \n",
      "void at::native::(anonymous namespace)::multi_tensor...         0.00%       0.000us         0.00%       0.000us       0.000us     280.000us         0.52%     280.000us      31.111us             9  \n",
      "                                          SiluBackward0         0.04%      61.000us         0.26%     363.000us      19.105us       0.000us         0.00%     273.000us      14.368us            19  \n",
      "void cutlass_cudnn_train::Kernel<cutlass_tensorop_s1...         0.00%       0.000us         0.00%       0.000us       0.000us     259.000us         0.48%     259.000us      37.000us             7  \n",
      "sm80_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     257.000us         0.48%     257.000us      85.667us             3  \n",
      "     autograd::engine::evaluate_function: MeanBackward1         0.23%     314.000us         1.24%       1.704ms      44.842us       0.000us         0.00%     252.000us       6.632us            38  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_6...         0.00%       0.000us         0.00%       0.000us       0.000us     237.000us         0.44%     237.000us      26.333us             9  \n",
      "                                           aten::linear         0.05%      70.000us         1.05%       1.444ms      68.762us       0.000us         0.00%     233.000us      11.095us            21  \n",
      "                                            aten::addmm         0.60%     831.000us         0.82%       1.129ms      53.762us     233.000us         0.44%     233.000us      11.095us            21  \n",
      "                        ampere_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us     233.000us         0.44%     233.000us      11.095us            21  \n",
      "    autograd::engine::evaluate_function: RsqrtBackward0         0.15%     202.000us         1.63%       2.241ms      58.974us       0.000us         0.00%     228.000us       6.000us            38  \n",
      "                                         RsqrtBackward0         0.23%     316.000us         1.48%       2.039ms      53.658us       0.000us         0.00%     228.000us       6.000us            38  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     225.000us         0.42%     225.000us       2.394us            94  \n",
      "void cutlass_cudnn_infer::Kernel<cutlass_tensorop_s1...         0.00%       0.000us         0.00%       0.000us       0.000us     225.000us         0.42%     225.000us      37.500us             6  \n",
      "void magma_sgemmEx_kernel<float, float, float, false...         0.00%       0.000us         0.00%       0.000us       0.000us     219.000us         0.41%     219.000us      18.250us            12  \n",
      "                                aten::_foreach_addcdiv_         0.18%     250.000us         0.21%     287.000us     287.000us     219.000us         0.41%     219.000us     219.000us             1  \n",
      "void at::native::(anonymous namespace)::multi_tensor...         0.00%       0.000us         0.00%       0.000us       0.000us     219.000us         0.41%     219.000us      36.500us             6  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 137.539ms\n",
      "Self CUDA time total: 53.456ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by='cuda_time_total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cbe9667-03f6-4211-b42a-abf8233a0007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        34.51%      47.469ms        34.53%      47.492ms      47.492ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel         9.91%      13.632ms         9.91%      13.632ms       5.418us       0.000us         0.00%       0.000us       0.000us          2516  \n",
      "                              Optimizer.step#AdamW.step         4.91%       6.758ms         9.54%      13.120ms      13.120ms       0.000us         0.00%       1.226ms       1.226ms             1  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.62%     855.000us         9.27%      12.747ms     169.960us       0.000us         0.00%      17.319ms     230.920us            75  \n",
      "                                   ConvolutionBackward0         0.30%     406.000us         8.49%      11.674ms     155.653us       0.000us         0.00%      16.942ms     225.893us            75  \n",
      "                             aten::convolution_backward         4.51%       6.208ms         8.19%      11.268ms     150.240us      15.426ms        28.86%      16.942ms     225.893us            75  \n",
      "                                           aten::conv2d         0.28%     384.000us         5.67%       7.794ms     103.920us       0.000us         0.00%       6.198ms      82.640us            75  \n",
      "                                      aten::convolution         0.69%     948.000us         5.52%       7.593ms     101.240us       0.000us         0.00%       6.295ms      83.933us            75  \n",
      "                                               aten::to         2.90%       3.984ms         5.39%       7.414ms      23.314us       0.000us         0.00%     116.000us       0.365us           318  \n",
      "                                     aten::_convolution         0.53%     733.000us         4.83%       6.645ms      88.600us       0.000us         0.00%       6.295ms      83.933us            75  \n",
      "autograd::engine::evaluate_function: NativeGroupNorm...         0.40%     549.000us         4.45%       6.117ms     130.149us       0.000us         0.00%       4.033ms      85.809us            47  \n",
      "                               NativeGroupNormBackward0         0.26%     355.000us         3.97%       5.465ms     116.277us       0.000us         0.00%       3.921ms      83.426us            47  \n",
      "                                              aten::mul         2.44%       3.360ms         3.71%       5.107ms      14.467us       1.719ms         3.22%       1.719ms       4.870us           353  \n",
      "                       aten::native_group_norm_backward         1.66%       2.284ms         3.60%       4.954ms     105.404us       3.873ms         7.25%       3.873ms      82.404us            47  \n",
      "                                aten::cudnn_convolution         2.51%       3.454ms         3.40%       4.675ms      62.333us       5.434ms        10.17%       5.434ms      72.453us            75  \n",
      "                                       aten::group_norm         0.12%     165.000us         2.95%       4.053ms      86.234us       0.000us         0.00%       1.670ms      35.532us            47  \n",
      "                                            aten::copy_         0.34%     474.000us         2.85%       3.920ms      11.632us       2.464ms         4.61%       2.464ms       7.312us           337  \n",
      "                                aten::native_group_norm         1.47%       2.016ms         2.83%       3.888ms      82.723us       1.670ms         3.12%       1.670ms      35.532us            47  \n",
      "                                             aten::add_         1.35%       1.859ms         2.71%       3.731ms       7.093us       1.624ms         3.04%       1.624ms       3.087us           526  \n",
      "                                              aten::sum         1.88%       2.591ms         2.71%       3.723ms      18.522us       2.829ms         5.29%       2.829ms      14.075us           201  \n",
      "                    Optimizer.zero_grad#AdamW.zero_grad         2.66%       3.663ms         2.66%       3.663ms       3.663ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                         aten::_to_copy         0.39%     538.000us         2.61%       3.588ms      12.906us       0.000us         0.00%     116.000us       0.417us           278  \n",
      "      autograd::engine::evaluate_function: MulBackward0         0.44%     607.000us         2.49%       3.418ms      51.788us       0.000us         0.00%       1.495ms      22.652us            66  \n",
      "                                            aten::empty         2.41%       3.320ms         2.41%       3.320ms       4.104us       0.000us         0.00%       0.000us       0.000us           809  \n",
      "      autograd::engine::evaluate_function: VarBackward0         0.23%     312.000us         2.23%       3.071ms      80.816us       0.000us         0.00%       1.132ms      29.789us            38  \n",
      "                                        cudaMemcpyAsync         2.16%       2.969ms         2.16%       2.969ms       1.484ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                          aten::reshape         0.64%     875.000us         1.94%       2.670ms       9.745us       0.000us         0.00%       2.138ms       7.803us           274  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.22%     306.000us         1.82%       2.501ms     119.095us       0.000us         0.00%     615.000us      29.286us            21  \n",
      "                                           VarBackward0         0.27%     368.000us         1.77%       2.431ms      63.974us       0.000us         0.00%       1.030ms      27.105us            38  \n",
      "    autograd::engine::evaluate_function: RsqrtBackward0         0.15%     202.000us         1.63%       2.241ms      58.974us       0.000us         0.00%     228.000us       6.000us            38  \n",
      "                                              aten::add         1.07%       1.478ms         1.56%       2.148ms      18.842us     899.000us         1.68%     899.000us       7.886us           114  \n",
      "                                    aten::_foreach_add_         0.42%     575.000us         1.52%       2.086ms       1.043ms     153.000us         0.29%     153.000us      76.500us             2  \n",
      "                                         RsqrtBackward0         0.23%     316.000us         1.48%       2.039ms      53.658us       0.000us         0.00%     228.000us       6.000us            38  \n",
      "                                           aten::matmul         0.14%     188.000us         1.45%       1.994ms     110.778us       0.000us         0.00%       5.111ms     283.944us            18  \n",
      "                                           MulBackward0         0.21%     282.000us         1.43%       1.962ms      29.727us       0.000us         0.00%     777.000us      11.773us            66  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.62%     848.000us         1.41%       1.935ms       6.986us       0.000us         0.00%       0.000us       0.000us           277  \n",
      "                                    aten::empty_strided         1.31%       1.805ms         1.31%       1.805ms       2.162us       0.000us         0.00%       0.000us       0.000us           835  \n",
      "                                             aten::mean         0.89%       1.222ms         1.28%       1.755ms      22.792us       1.275ms         2.39%       1.275ms      16.558us            77  \n",
      "                                            aten::clone         0.24%     332.000us         1.25%       1.716ms      29.085us       0.000us         0.00%       2.114ms      35.831us            59  \n",
      "     autograd::engine::evaluate_function: MeanBackward1         0.23%     314.000us         1.24%       1.704ms      44.842us       0.000us         0.00%     252.000us       6.632us            38  \n",
      "                                         AddmmBackward0         0.14%     187.000us         1.14%       1.572ms      74.857us       0.000us         0.00%     392.000us      18.667us            21  \n",
      "                                     aten::_foreach_add         0.46%     636.000us         1.10%       1.519ms       1.519ms     129.000us         0.24%     129.000us     129.000us             1  \n",
      "                                           aten::linear         0.05%      70.000us         1.05%       1.444ms      68.762us       0.000us         0.00%     233.000us      11.095us            21  \n",
      "                                              aten::bmm         0.75%       1.037ms         1.04%       1.434ms      26.556us      10.142ms        18.97%      10.142ms     187.815us            54  \n",
      "                                    aten::_foreach_sqrt         0.35%     481.000us         1.01%       1.391ms       1.391ms     145.000us         0.27%     145.000us     145.000us             1  \n",
      "      autograd::engine::evaluate_function: SubBackward0         0.20%     270.000us         0.98%       1.346ms      35.421us       0.000us         0.00%     517.000us      13.605us            38  \n",
      "                                              aten::sub         0.67%     924.000us         0.95%       1.300ms      17.105us     320.000us         0.60%     320.000us       4.211us            76  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.09%     121.000us         0.90%       1.241ms      68.944us       0.000us         0.00%       6.088ms     338.222us            18  \n",
      "                                            aten::addmm         0.60%     831.000us         0.82%       1.129ms      53.762us     233.000us         0.44%     233.000us      11.095us            21  \n",
      "                                               aten::mm         0.56%     766.000us         0.82%       1.129ms      27.537us     392.000us         0.73%     392.000us       9.561us            41  \n",
      "                                           BmmBackward0         0.10%     135.000us         0.80%       1.104ms      61.333us       0.000us         0.00%       5.980ms     332.222us            18  \n",
      "                                              aten::var         0.40%     548.000us         0.79%       1.091ms      28.711us     886.000us         1.66%     886.000us      23.316us            38  \n",
      "                                          MeanBackward1         0.11%     153.000us         0.79%       1.083ms      28.500us       0.000us         0.00%     150.000us       3.947us            38  \n",
      "                        torch::autograd::AccumulateGrad         0.28%     391.000us         0.72%     992.000us       3.581us       0.000us         0.00%       0.000us       0.000us           277  \n",
      "      autograd::engine::evaluate_function: AddBackward0         0.33%     448.000us         0.68%     932.000us       8.962us       0.000us         0.00%     322.000us       3.096us           104  \n",
      "                                              aten::cat         0.38%     521.000us         0.58%     795.000us      20.921us       1.397ms         2.61%       1.397ms      36.763us            38  \n",
      "autograd::engine::evaluate_function: ReshapeAliasBac...         0.13%     173.000us         0.57%     784.000us      13.517us       0.000us         0.00%     633.000us      10.914us            58  \n",
      "    autograd::engine::evaluate_function: SplitBackward0         0.11%     149.000us         0.55%     755.000us      26.964us       0.000us         0.00%       1.111ms      39.679us            28  \n",
      "                                              aten::div         0.38%     517.000us         0.55%     754.000us      19.842us     150.000us         0.28%     150.000us       3.947us            38  \n",
      "                                              aten::pow         0.35%     475.000us         0.51%     701.000us      18.447us      76.000us         0.14%      76.000us       2.000us            38  \n",
      "                                            aten::rsqrt         0.36%     493.000us         0.49%     679.000us      17.868us      76.000us         0.14%      76.000us       2.000us            38  \n",
      "                                           aten::detach         0.23%     323.000us         0.47%     644.000us       2.325us       0.000us         0.00%       0.000us       0.000us           277  \n",
      "                                    aten::_foreach_mul_         0.42%     578.000us         0.46%     638.000us     212.667us     280.000us         0.52%     280.000us      93.333us             3  \n",
      "                                         SplitBackward0         0.05%      71.000us         0.44%     606.000us      21.643us       0.000us         0.00%       1.111ms      39.679us            28  \n",
      "                                        aten::transpose         0.36%     491.000us         0.43%     585.000us       3.324us       0.000us         0.00%       0.000us       0.000us           176  \n",
      "                                                aten::t         0.20%     272.000us         0.41%     566.000us       5.442us       0.000us         0.00%       0.000us       0.000us           104  \n",
      "                                  ReshapeAliasBackward0         0.08%     110.000us         0.41%     566.000us       9.759us       0.000us         0.00%     633.000us      10.914us            58  \n",
      "                                           SubBackward0         0.04%      59.000us         0.40%     544.000us      14.316us       0.000us         0.00%     101.000us       2.658us            38  \n",
      "                                        cudaMemsetAsync         0.39%     542.000us         0.39%     542.000us       5.956us       0.000us         0.00%       0.000us       0.000us            91  \n",
      "                                            aten::chunk         0.09%     130.000us         0.38%     527.000us      18.821us       0.000us         0.00%       0.000us       0.000us            28  \n",
      "                                       aten::empty_like         0.07%     102.000us         0.38%     523.000us       8.574us       0.000us         0.00%       0.000us       0.000us            61  \n",
      "     autograd::engine::evaluate_function: SiluBackward0         0.11%     145.000us         0.38%     523.000us      27.526us       0.000us         0.00%     280.000us      14.737us            19  \n",
      "                                              aten::neg         0.21%     289.000us         0.35%     485.000us      12.763us     101.000us         0.19%     101.000us       2.658us            38  \n",
      "                                            aten::split         0.13%     176.000us         0.35%     478.000us      17.071us       0.000us         0.00%       0.000us       0.000us            28  \n",
      "                                   aten::_reshape_alias         0.32%     436.000us         0.32%     436.000us       1.904us       0.000us         0.00%       0.000us       0.000us           229  \n",
      "autograd::engine::evaluate_function: UnsafeViewBackw...         0.12%     162.000us         0.32%     436.000us       8.074us       0.000us         0.00%       0.000us       0.000us            54  \n",
      "                                       aten::contiguous         0.01%      13.000us         0.31%     431.000us      33.154us       0.000us         0.00%     191.000us      14.692us            13  \n",
      "                                           aten::narrow         0.12%     161.000us         0.30%     408.000us       4.916us       0.000us         0.00%       0.000us       0.000us            83  \n",
      "autograd::engine::evaluate_function: SoftmaxBackward...         0.04%      53.000us         0.29%     393.000us      43.667us       0.000us         0.00%      92.000us      10.222us             9  \n",
      "                                                 detach         0.27%     373.000us         0.27%     373.000us       1.347us       0.000us         0.00%       0.000us       0.000us           277  \n",
      "                                          SiluBackward0         0.04%      61.000us         0.26%     363.000us      19.105us       0.000us         0.00%     273.000us      14.368us            19  \n",
      "                                           aten::expand         0.20%     277.000us         0.25%     346.000us       4.676us       0.000us         0.00%       0.000us       0.000us            74  \n",
      "                                       SoftmaxBackward0         0.02%      32.000us         0.25%     340.000us      37.778us       0.000us         0.00%      92.000us      10.222us             9  \n",
      "                                    aten::silu_backward         0.14%     197.000us         0.23%     317.000us      16.684us     280.000us         0.52%     280.000us      14.737us            19  \n",
      "                                       aten::as_strided         0.23%     314.000us         0.23%     314.000us       0.777us       0.000us         0.00%       0.000us       0.000us           404  \n",
      "                           aten::_softmax_backward_data         0.08%     115.000us         0.22%     308.000us      34.222us      45.000us         0.08%      92.000us      10.222us             9  \n",
      "                                             aten::silu         0.14%     198.000us         0.22%     301.000us      15.842us     162.000us         0.30%     162.000us       8.526us            19  \n",
      "                                                INVALID         0.22%     297.000us         0.22%     297.000us       4.950us       0.000us         0.00%       0.000us       0.000us            60  \n",
      "                                aten::_foreach_addcdiv_         0.18%     250.000us         0.21%     287.000us     287.000us     219.000us         0.41%     219.000us     219.000us             1  \n",
      "                                          aten::resize_         0.20%     277.000us         0.20%     277.000us       3.221us       0.000us         0.00%       0.000us       0.000us            86  \n",
      "                                   cudaFuncSetAttribute         0.19%     267.000us         0.19%     267.000us       1.085us       0.000us         0.00%       0.000us       0.000us           246  \n",
      "                                            aten::slice         0.15%     205.000us         0.19%     263.000us       3.169us       0.000us         0.00%       0.000us       0.000us            83  \n",
      "                                             aten::view         0.19%     260.000us         0.19%     260.000us       0.446us       0.000us         0.00%       0.000us       0.000us           583  \n",
      "                                    UnsafeViewBackward0         0.07%      90.000us         0.19%     259.000us       4.796us       0.000us         0.00%       0.000us       0.000us            54  \n",
      "                                aten::_foreach_addcmul_         0.15%     204.000us         0.17%     238.000us     238.000us     178.000us         0.33%     178.000us     178.000us             1  \n",
      "autograd::engine::evaluate_function: SmoothL1LossBac...         0.04%      59.000us         0.17%     233.000us     233.000us       0.000us         0.00%       9.000us       9.000us             1  \n",
      "                                    aten::_foreach_div_         0.13%     175.000us         0.14%     197.000us     197.000us     122.000us         0.23%     122.000us     122.000us             1  \n",
      "                                          aten::softmax         0.01%      16.000us         0.14%     193.000us      21.444us       0.000us         0.00%      63.000us       7.000us             9  \n",
      "      autograd::engine::evaluate_function: CatBackward0         0.02%      34.000us         0.13%     185.000us      20.556us       0.000us         0.00%       0.000us       0.000us             9  \n",
      "                                     aten::_unsafe_view         0.13%     177.000us         0.13%     177.000us       2.810us       0.000us         0.00%       0.000us       0.000us            63  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 137.539ms\n",
      "Self CUDA time total: 53.456ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by='cpu_time_total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037a9ea-3377-486c-8d7b-0b24808f09a9",
   "metadata": {},
   "source": [
    "### Measure Time with CUDA Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b04d7b4e-8755-4e4d-a1ab-8914328407f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.03197265625\n"
     ]
    }
   ],
   "source": [
    "ddpm.train()\n",
    "\n",
    "start_e = torch.cuda.Event(enable_timing=True)\n",
    "end_e = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_e.record()\n",
    "\n",
    "for _, (x0, _) in zip(range(100), dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x0 = x0.to('cuda')\n",
    "    eps = torch.randn_like(x0)\n",
    "    t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "    \n",
    "    eps_pred = ddpm(x0, eps, t)\n",
    "    loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "end_e.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "t = start_e.elapsed_time(end_e) / 100.0\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2041d96c-9d17-402d-931c-68750ea7ea67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9137593551387764"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflop_per_sec = (183.66 / (t / 1000)) / 1000\n",
    "tflop_per_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a5707-92ff-4ebd-b96c-19151239af61",
   "metadata": {},
   "source": [
    "A100 has 19.5 TFLOP/s of FP32 compute ([Spec](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59f05130-8a39-48b0-8734-35e2d0973be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfu=14.94%\n"
     ]
    }
   ],
   "source": [
    "mfu = tflop_per_sec / 19.5\n",
    "print(f'{mfu=:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99533b0-1260-4df3-aed6-687ee7bc119f",
   "metadata": {},
   "source": [
    "Wow 14.94% MFU is pretty bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f348d-e137-438b-b438-c8095df0fec5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Optimization 1: Scale Up Batch Size\n",
    "\n",
    "Scaling up to batch size 1024 roughly saturates the FP32 compute, but still leaves FP16 compute on the table.  \n",
    "20.84 TFLOP/s per iteration (7.16x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54fe2193-83a9-41a3-bb0a-b8219bd49331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "img2tensor = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=0.5, std=0.5)  # [0, 1] -> [-1, 1]\n",
    "])\n",
    "ds = CIFAR10('./cifar10', train=True, transform=img2tensor, download=True)\n",
    "cfg_m = ModelConfig()\n",
    "cfg_t = TrainerConfig(bs=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f7a06d5-0f0f-4c8c-823d-72869a5a6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ds, batch_size=cfg_t.bs, num_workers=cfg_t.num_workers, drop_last=True)\n",
    "ddpm = DDPM(**asdict(cfg_m)).to('cuda')\n",
    "optimizer = torch.optim.AdamW(ddpm.parameters(), lr=cfg_t.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b2a9b0-7721-4390-8e3e-6df7f9225afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, _ = next(iter(dataloader))\n",
    "x0 = x0.to('cuda')\n",
    "eps = torch.randn_like(x0)\n",
    "t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45a443a5-24c9-4224-8d80-b7f4b3fedefc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "DDPM                                          [1024, 3, 32, 32]         --\n",
       "├─UNet: 1-1                                   [1024, 3, 32, 32]         --\n",
       "│    └─Sequential: 2-1                        [1024, 128]               --\n",
       "│    │    └─Linear: 3-1                       [1024, 128]               4,224\n",
       "│    │    └─GELU: 3-2                         [1024, 128]               --\n",
       "│    │    └─Linear: 3-3                       [1024, 128]               16,512\n",
       "│    └─Conv2d: 2-2                            [1024, 32, 32, 32]        128\n",
       "│    └─UNetDownsample: 2-3                    [1024, 32, 16, 16]        --\n",
       "│    │    └─TimeResNetBlock: 3-4              [1024, 32, 32, 32]        26,880\n",
       "│    │    └─TimeResNetBlock: 3-5              [1024, 32, 32, 32]        26,880\n",
       "│    │    └─GroupNorm: 3-6                    [1024, 32, 32, 32]        64\n",
       "│    │    └─Attention: 3-7                    [1024, 32, 32, 32]        16,416\n",
       "│    │    └─DownsampleOutProject: 3-8         [1024, 32, 16, 16]        4,128\n",
       "│    └─UNetDownsample: 2-4                    [1024, 64, 8, 8]          --\n",
       "│    │    └─TimeResNetBlock: 3-9              [1024, 32, 16, 16]        26,880\n",
       "│    │    └─TimeResNetBlock: 3-10             [1024, 32, 16, 16]        26,880\n",
       "│    │    └─GroupNorm: 3-11                   [1024, 32, 16, 16]        64\n",
       "│    │    └─Attention: 3-12                   [1024, 32, 16, 16]        16,416\n",
       "│    │    └─DownsampleOutProject: 3-13        [1024, 64, 8, 8]          8,256\n",
       "│    └─UNetDownsample: 2-5                    [1024, 128, 4, 4]         --\n",
       "│    │    └─TimeResNetBlock: 3-14             [1024, 64, 8, 8]          90,624\n",
       "│    │    └─TimeResNetBlock: 3-15             [1024, 64, 8, 8]          90,624\n",
       "│    │    └─GroupNorm: 3-16                   [1024, 64, 8, 8]          128\n",
       "│    │    └─Attention: 3-17                   [1024, 64, 8, 8]          32,832\n",
       "│    │    └─DownsampleOutProject: 3-18        [1024, 128, 4, 4]         32,896\n",
       "│    └─UNetDownsample: 2-6                    [1024, 256, 4, 4]         --\n",
       "│    │    └─TimeResNetBlock: 3-19             [1024, 128, 4, 4]         328,704\n",
       "│    │    └─TimeResNetBlock: 3-20             [1024, 128, 4, 4]         328,704\n",
       "│    │    └─GroupNorm: 3-21                   [1024, 128, 4, 4]         256\n",
       "│    │    └─Attention: 3-22                   [1024, 128, 4, 4]         65,664\n",
       "│    │    └─Conv2d: 3-23                      [1024, 256, 4, 4]         295,168\n",
       "│    └─UNetBlock: 2-7                         [1024, 256, 4, 4]         --\n",
       "│    │    └─TimeResNetBlock: 3-24             [1024, 256, 4, 4]         1,247,232\n",
       "│    │    └─TimeResNetBlock: 3-25             [1024, 256, 4, 4]         1,247,232\n",
       "│    │    └─GroupNorm: 3-26                   [1024, 256, 4, 4]         512\n",
       "│    │    └─Attention: 3-27                   [1024, 256, 4, 4]         131,328\n",
       "│    └─UNetUpsample: 2-8                      [1024, 128, 8, 8]         --\n",
       "│    │    └─TimeResNetBlock: 3-28             [1024, 256, 4, 4]         1,640,704\n",
       "│    │    └─TimeResNetBlock: 3-29             [1024, 256, 4, 4]         1,640,704\n",
       "│    │    └─GroupNorm: 3-30                   [1024, 256, 4, 4]         512\n",
       "│    │    └─Attention: 3-31                   [1024, 256, 4, 4]         131,328\n",
       "│    │    └─Sequential: 3-32                  [1024, 128, 8, 8]         295,040\n",
       "│    └─UNetUpsample: 2-9                      [1024, 64, 16, 16]        --\n",
       "│    │    └─TimeResNetBlock: 3-33             [1024, 128, 8, 8]         427,136\n",
       "│    │    └─TimeResNetBlock: 3-34             [1024, 128, 8, 8]         427,136\n",
       "│    │    └─GroupNorm: 3-35                   [1024, 128, 8, 8]         256\n",
       "│    │    └─Attention: 3-36                   [1024, 128, 8, 8]         65,664\n",
       "│    │    └─Sequential: 3-37                  [1024, 64, 16, 16]        73,792\n",
       "│    └─UNetUpsample: 2-10                     [1024, 32, 32, 32]        --\n",
       "│    │    └─TimeResNetBlock: 3-38             [1024, 64, 16, 16]        115,264\n",
       "│    │    └─TimeResNetBlock: 3-39             [1024, 64, 16, 16]        115,264\n",
       "│    │    └─GroupNorm: 3-40                   [1024, 64, 16, 16]        128\n",
       "│    │    └─Attention: 3-41                   [1024, 64, 16, 16]        32,832\n",
       "│    │    └─Sequential: 3-42                  [1024, 32, 32, 32]        18,464\n",
       "│    └─UNetUpsample: 2-11                     [1024, 32, 32, 32]        --\n",
       "│    │    └─TimeResNetBlock: 3-43             [1024, 32, 32, 32]        38,176\n",
       "│    │    └─TimeResNetBlock: 3-44             [1024, 32, 32, 32]        38,176\n",
       "│    │    └─GroupNorm: 3-45                   [1024, 32, 32, 32]        64\n",
       "│    │    └─Attention: 3-46                   [1024, 32, 32, 32]        16,416\n",
       "│    │    └─Conv2d: 3-47                      [1024, 32, 32, 32]        9,248\n",
       "│    └─TimeResNetBlock: 2-12                  [1024, 32, 32, 32]        --\n",
       "│    │    └─Linear: 3-48                      [1024, 64]                8,256\n",
       "│    │    └─Sequential: 3-49                  [1024, 32, 32, 32]        18,528\n",
       "│    │    └─Sequential: 3-50                  [1024, 32, 32, 32]        9,312\n",
       "│    │    └─Conv2d: 3-51                      [1024, 32, 32, 32]        2,080\n",
       "│    └─Conv2d: 2-13                           [1024, 3, 32, 32]         99\n",
       "===============================================================================================\n",
       "Total params: 9,190,211\n",
       "Trainable params: 9,190,211\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 489.73\n",
       "===============================================================================================\n",
       "Input size (MB): 25.17\n",
       "Forward/backward pass size (MB): 21400.91\n",
       "Params size (MB): 36.76\n",
       "Estimated Total Size (MB): 21462.85\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(ddpm, input_data=[x0, eps, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35316ca5-3175-440b-bfd5-e1d49dc86016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.93838"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflops_per_iter = (489.73 * 2 * 3) / 1000\n",
    "tflops_per_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80735197-59ec-4049-a345-032144b41c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.0171875\n"
     ]
    }
   ],
   "source": [
    "ddpm.train()\n",
    "\n",
    "start_e = torch.cuda.Event(enable_timing=True)\n",
    "end_e = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_e.record()\n",
    "\n",
    "for _, (x0, _) in zip(range(100), dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x0 = x0.to('cuda')\n",
    "    eps = torch.randn_like(x0)\n",
    "    t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "    \n",
    "    eps_pred = ddpm(x0, eps, t)\n",
    "    loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "end_e.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "t = start_e.elapsed_time(end_e) / 100.0\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ecd5954-a63d-4f7d-9183-fe65983c9dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.837034492692602"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflop_per_sec = tflops_per_iter / (t / 1000)\n",
    "tflop_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2bd31d0-470c-4c17-bf98-cca7c9597f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfu=106.86%\n"
     ]
    }
   ],
   "source": [
    "mfu = tflop_per_sec / A100_FP32_TFLOPS_PER_SEC\n",
    "print(f'{mfu=:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe3778-3169-4427-a35e-8936698a7bf2",
   "metadata": {},
   "source": [
    "**All parameters are in either FP32 or INT32 format.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a433ce3-c964-48db-b23e-f66b67565081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(p.dtype in [torch.float32, torch.int32] for p in ddpm.model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3e0e6-fca2-484e-9650-afdcf7dc7ae6",
   "metadata": {},
   "source": [
    "Profiling shows that CPU is no longer an overhead with this batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4a9cf38-20be-4ae7-944c-14a8e5489133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function\n",
    "from torch.profiler import ProfilerActivity as PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c44823f4-b64b-4f39-80f7-4c3f73355c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-06-18 08:00:15 1518246:1518246 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-06-18 08:00:30 1518246:1518246 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-06-18 08:00:30 1518246:1518246 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "ddpm.train()\n",
    "\n",
    "with profile(activities=[PA.CPU, PA.CUDA], record_shapes=True) as prof:\n",
    "    for _, (x0, _) in zip(range(100), dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x0 = x0.to('cuda')\n",
    "        eps = torch.randn_like(x0)\n",
    "        t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "        \n",
    "        eps_pred = ddpm(x0, eps, t)\n",
    "        loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61f25499-4922-480e-8a58-8634db60c8ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.33%      41.929ms        19.85%        2.547s     707.427us       0.000us         0.00%        5.017s       1.394ms          3600  \n",
      "                             aten::convolution_backward         2.25%     288.713ms        18.96%        2.432s     675.461us        4.157s        32.31%        4.855s       1.349ms          3600  \n",
      "                                   ConvolutionBackward0         0.15%      19.680ms        19.10%        2.451s     680.714us       0.000us         0.00%        4.847s       1.346ms          3600  \n",
      "                                              aten::bmm         0.34%      43.852ms         1.36%     174.513ms      67.328us        2.847s        22.13%        2.865s       1.105ms          2592  \n",
      "                                      aten::convolution         0.19%      24.785ms         2.32%     297.271ms      82.575us       0.000us         0.00%        1.856s     515.420us          3600  \n",
      "                                     aten::_convolution         0.24%      30.787ms         2.12%     272.345ms      75.651us       0.000us         0.00%        1.855s     515.193us          3600  \n",
      "                                           aten::conv2d         0.10%      12.341ms         2.39%     306.982ms      85.273us       0.000us         0.00%        1.841s     511.394us          3600  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.05%       6.461ms         1.33%     170.907ms     197.809us       0.000us         0.00%        1.717s       1.988ms           864  \n",
      "                                           BmmBackward0         0.06%       7.101ms         1.28%     163.992ms     189.806us       0.000us         0.00%        1.695s       1.962ms           864  \n",
      "                                aten::cudnn_convolution         1.07%     137.581ms         1.48%     189.643ms      52.679us        1.406s        10.93%        1.560s     433.345us          3600  \n",
      "                                           aten::matmul         0.07%       8.598ms         0.63%      81.345ms      94.149us       0.000us         0.00%        1.490s       1.725ms           864  \n",
      "_ZN19cutlass_cudnn_train6KernelINS_4conv6kernel23Imp...         0.00%       0.000us         0.00%       0.000us       0.000us        1.114s         8.66%        1.114s       1.934ms           576  \n",
      "                                ampere_sgemm_128x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us        1.089s         8.47%        1.089s       3.783ms           288  \n",
      "                                ampere_sgemm_128x128_tt         0.00%       0.000us         0.00%       0.000us       0.000us     973.352ms         7.57%     973.352ms       3.380ms           288  \n",
      "void cudnn::ops::nchwToNhwcKernel<float, float, floa...         0.00%       0.000us         0.00%       0.000us       0.000us     906.779ms         7.05%     906.779ms      65.595us         13824  \n",
      "                                       cudaLaunchKernel        47.03%        6.032s        47.03%        6.032s      49.851us     897.041ms         6.97%     897.054ms       7.413us        121008  \n",
      "autograd::engine::evaluate_function: NativeGroupNorm...         0.21%      27.134ms         9.29%        1.192s     528.495us       0.000us         0.00%     859.362ms     380.923us          2256  \n",
      "                               NativeGroupNormBackward0         0.13%      17.096ms         8.58%        1.101s     488.111us       0.000us         0.00%     809.680ms     358.901us          2256  \n",
      "                                            aten::copy_         0.16%      20.969ms        22.50%        2.886s     178.439us     788.119ms         6.13%     806.185ms      49.838us         16176  \n",
      "                       aten::native_group_norm_backward         0.93%     119.895ms         8.41%        1.079s     478.270us     706.564ms         5.49%     792.216ms     351.160us          2256  \n",
      "                                            aten::clone         0.09%      11.739ms         0.60%      77.548ms      27.383us       0.000us         0.00%     724.739ms     255.911us          2832  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     703.091ms         5.46%     703.091ms     252.547us          2784  \n",
      "                                          aten::reshape         0.31%      39.654ms         0.88%     113.048ms       8.595us       0.000us         0.00%     653.851ms      49.715us         13152  \n",
      "                                             aten::add_         0.65%      82.998ms         3.63%     466.164ms      18.427us     498.300ms         3.87%     557.339ms      22.031us         25298  \n",
      "                                              aten::cat         0.19%      23.981ms         0.44%      55.912ms      30.654us     463.968ms         3.61%     478.282ms     262.216us          1824  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     463.824ms         3.61%     463.824ms     261.162us          1776  \n",
      "                                              aten::mul         1.29%     165.649ms        11.67%        1.497s      88.257us     318.811ms         2.48%     459.172ms      27.074us         16960  \n",
      "                                       aten::group_norm         0.05%       6.506ms         1.30%     166.770ms      73.923us       0.000us         0.00%     457.761ms     202.908us          2256  \n",
      "                                aten::native_group_norm         0.58%      74.854ms         1.25%     160.264ms      71.039us     419.709ms         3.26%     457.761ms     202.908us          2256  \n",
      "                                              aten::sum         1.06%     136.031ms         6.69%     857.739ms      88.903us     350.799ms         2.73%     438.577ms      45.458us          9648  \n",
      "sm80_xmma_wgrad_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     423.935ms         3.30%     423.935ms     735.998us           576  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     418.474ms         3.25%     418.474ms      50.984us          8208  \n",
      "    autograd::engine::evaluate_function: SplitBackward0         0.06%       8.065ms         0.45%      57.487ms      42.773us       0.000us         0.00%     380.280ms     282.946us          1344  \n",
      "                                         SplitBackward0         0.03%       3.889ms         0.38%      49.343ms      36.714us       0.000us         0.00%     380.257ms     282.929us          1344  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     374.254ms         2.91%     374.254ms      51.980us          7200  \n",
      "sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     373.772ms         2.91%     373.772ms     486.682us           768  \n",
      "                                ampere_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us     361.604ms         2.81%     361.604ms     837.046us           432  \n",
      "      autograd::engine::evaluate_function: MulBackward0         0.22%      27.754ms         5.29%     678.725ms     214.244us       0.000us         0.00%     343.623ms     108.467us          3168  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     336.420ms         2.61%     336.420ms      38.938us          8640  \n",
      "void at::native::(anonymous namespace)::ComputeInter...         0.00%       0.000us         0.00%       0.000us       0.000us     329.907ms         2.56%     329.907ms     146.235us          2256  \n",
      "                                              aten::add         0.49%      62.286ms         0.96%     123.176ms      22.510us     288.686ms         2.24%     322.401ms      58.918us          5472  \n",
      "void cudnn::ops::nhwcToNchwKernel<float, float, floa...         0.00%       0.000us         0.00%       0.000us       0.000us     305.993ms         2.38%     305.993ms      56.918us          5376  \n",
      "                                ampere_sgemm_128x128_nt         0.00%       0.000us         0.00%       0.000us       0.000us     305.188ms         2.37%     305.188ms       1.060ms           288  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_2...         0.00%       0.000us         0.00%       0.000us       0.000us     280.686ms         2.18%     280.686ms     417.688us           672  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     258.054ms         2.01%     258.054ms     114.386us          2256  \n",
      "cudnn_infer_ampere_scudnn_winograd_128x128_ldg1_ldg4...         0.00%       0.000us         0.00%       0.000us       0.000us     248.986ms         1.94%     248.986ms     648.401us           384  \n",
      "                                           MulBackward0         0.15%      19.403ms         3.25%     416.312ms     131.412us       0.000us         0.00%     245.986ms      77.647us          3168  \n",
      "void at::native::(anonymous namespace)::RowwiseMomen...         0.00%       0.000us         0.00%       0.000us       0.000us     227.882ms         1.77%     227.882ms     101.012us          2256  \n",
      "autograd::engine::evaluate_function: ReshapeAliasBac...         0.06%       7.777ms         0.29%      37.728ms      13.552us       0.000us         0.00%     186.725ms      67.071us          2784  \n",
      "autograd::engine::evaluate_function: UpsampleNearest...         0.00%     584.000us         0.03%       3.445ms      23.924us       0.000us         0.00%     184.358ms       1.280ms           144  \n",
      "                      aten::upsample_nearest2d_backward         0.01%       1.401ms         0.02%       2.426ms      16.847us     183.888ms         1.43%     184.358ms       1.280ms           144  \n",
      "void at::native::(anonymous namespace)::upsample_nea...         0.00%       0.000us         0.00%       0.000us       0.000us     183.888ms         1.43%     183.888ms       1.277ms           144  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     183.851ms         1.43%     183.851ms      81.494us          2256  \n",
      "                             UpsampleNearest2DBackward0         0.00%     435.000us         0.02%       2.812ms      19.528us       0.000us         0.00%     180.791ms       1.255ms           144  \n",
      "sm80_xmma_dgrad_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     180.618ms         1.40%     180.618ms     235.180us           768  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_6...         0.00%       0.000us         0.00%       0.000us       0.000us     180.145ms         1.40%     180.145ms     750.604us           240  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     175.882ms         1.37%     175.882ms      23.794us          7392  \n",
      "                                  ReshapeAliasBackward0         0.04%       5.669ms         0.22%      27.783ms       9.980us       0.000us         0.00%     169.866ms      61.015us          2784  \n",
      "void cutlass_cudnn_infer::Kernel<cutlass_tensorop_s1...         0.00%       0.000us         0.00%       0.000us       0.000us     168.297ms         1.31%     168.297ms     250.442us           672  \n",
      "sm80_xmma_wgrad_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     159.432ms         1.24%     159.432ms     221.433us           720  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_2...         0.00%       0.000us         0.00%       0.000us       0.000us     157.162ms         1.22%     157.162ms     272.851us           576  \n",
      "sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     139.961ms         1.09%     139.961ms     194.390us           720  \n",
      "_ZN19cutlass_cudnn_train6KernelINS_4conv6kernel23Imp...         0.00%       0.000us         0.00%       0.000us       0.000us     138.256ms         1.07%     138.256ms     576.067us           240  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     124.602ms         0.97%     124.602ms      24.960us          4992  \n",
      "sm80_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     122.892ms         0.96%     122.892ms       1.280ms            96  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_1...         0.00%       0.000us         0.00%       0.000us       0.000us     116.999ms         0.91%     116.999ms     221.589us           528  \n",
      "      autograd::engine::evaluate_function: VarBackward0         0.12%      15.513ms         6.76%     867.149ms     475.411us       0.000us         0.00%     113.197ms      62.060us          1824  \n",
      "     autograd::engine::evaluate_function: SiluBackward0         0.05%       6.191ms         0.28%      36.531ms      40.056us       0.000us         0.00%     110.199ms     120.832us           912  \n",
      "                                    aten::silu_backward         0.09%      11.440ms         0.21%      27.138ms      29.757us     105.103ms         0.82%     110.199ms     120.832us           912  \n",
      "                                          SiluBackward0         0.02%       3.202ms         0.23%      29.219ms      32.038us       0.000us         0.00%     105.436ms     115.610us           912  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     105.103ms         0.82%     105.103ms     115.245us           912  \n",
      "      autograd::engine::evaluate_function: AddBackward0         0.20%      26.120ms         0.94%     120.976ms      24.234us       0.000us         0.00%     104.906ms      21.015us          4992  \n",
      "                                           VarBackward0         0.14%      18.593ms         5.75%     737.565ms     404.367us       0.000us         0.00%      99.429ms      54.512us          1824  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.12%      15.045ms         2.43%     311.093ms     308.624us       0.000us         0.00%      98.130ms      97.351us          1008  \n",
      "                                             aten::mean         0.41%      52.924ms         1.18%     150.798ms      40.800us      52.370ms         0.41%      93.103ms      25.190us          3696  \n",
      "                                   cudaFuncSetAttribute         0.05%       5.795ms         0.05%       5.795ms       0.463us      92.036ms         0.72%      92.036ms       7.346us         12528  \n",
      "void at::native::(anonymous namespace)::GammaBetaBac...         0.00%       0.000us         0.00%       0.000us       0.000us      89.440ms         0.70%      89.440ms      39.645us          2256  \n",
      "                                  cudaStreamIsCapturing         0.00%     215.000us         0.00%     215.000us       0.020us      83.957ms         0.65%      83.957ms       7.705us         10896  \n",
      "                                             aten::silu         0.07%       9.050ms         0.10%      13.077ms      14.339us      69.167ms         0.54%      83.367ms      91.411us           912  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_6...         0.00%       0.000us         0.00%       0.000us       0.000us      81.660ms         0.63%      81.660ms     106.328us           768  \n",
      "                                         aten::_to_copy         0.22%      28.206ms        22.37%        2.870s     215.067us       0.000us         0.00%      75.770ms       5.678us         13344  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      75.175ms         0.58%      75.175ms       1.566ms            48  \n",
      "void cutlass_cudnn_train::Kernel<cutlass_tensorop_s1...         0.00%       0.000us         0.00%       0.000us       0.000us      73.698ms         0.57%      73.698ms     255.896us           288  \n",
      "                              Optimizer.step#AdamW.step         1.72%     221.205ms         4.14%     531.186ms      11.066ms       0.000us         0.00%      70.949ms       1.478ms            48  \n",
      "                                               aten::to         1.98%     254.491ms        22.44%        2.879s     188.589us       0.000us         0.00%      69.524ms       4.555us         15264  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      69.167ms         0.54%      69.167ms      75.841us           912  \n",
      "                                  cudaStreamGetPriority         0.00%      53.000us         0.00%      53.000us       0.005us      68.024ms         0.53%      68.024ms       6.327us         10752  \n",
      "                                       aten::contiguous         0.01%     834.000us         0.17%      21.876ms      35.058us       0.000us         0.00%      65.368ms     104.756us           624  \n",
      "void cudnn::cnn::reduce_wgrad_nchw_helper<float, flo...         0.00%       0.000us         0.00%       0.000us       0.000us      62.216ms         0.48%      62.216ms      64.808us           960  \n",
      "                                         AddmmBackward0         0.08%      10.170ms         1.33%     170.285ms     168.934us       0.000us         0.00%      61.994ms      61.502us          1008  \n",
      "                                               aten::mm         0.29%      37.404ms         1.15%     146.910ms      74.649us      22.963ms         0.18%      61.994ms      31.501us          1968  \n",
      "                       cudaDeviceGetStreamPriorityRange         0.00%       5.000us         0.00%       5.000us       0.000us      60.918ms         0.47%      60.918ms       5.666us         10752  \n",
      "_ZN19cutlass_cudnn_train6KernelINS_4conv6kernel23Imp...         0.00%       0.000us         0.00%       0.000us       0.000us      59.509ms         0.46%      59.509ms     413.257us           144  \n",
      "                                              aten::var         0.19%      24.561ms         0.36%      46.090ms      25.269us      33.055ms         0.26%      54.223ms      29.728us          1824  \n",
      "    autograd::engine::evaluate_function: RsqrtBackward0         0.08%      10.348ms         7.06%     905.753ms     496.575us       0.000us         0.00%      53.486ms      29.323us          1824  \n",
      "                                         RsqrtBackward0         0.11%      13.980ms         6.98%     895.405ms     490.902us       0.000us         0.00%      53.486ms      29.323us          1824  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      52.278ms         0.41%      52.278ms      14.144us          3696  \n",
      "void magma_sgemmEx_kernel<float, float, float, false...         0.00%       0.000us         0.00%       0.000us       0.000us      50.107ms         0.39%      50.107ms      86.991us           576  \n",
      "sm80_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us      48.276ms         0.38%      48.276ms     335.250us           144  \n",
      "                                              aten::sub         0.36%      46.006ms         1.56%     200.453ms      54.949us      11.800ms         0.09%      38.532ms      10.562us          3648  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 12.828s\n",
      "Self CUDA time total: 12.866s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by='cuda_time_total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17d8d812-167f-45bb-9200-0bfe7f6a3b7f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       cudaLaunchKernel        47.03%        6.032s        47.03%        6.032s      49.851us     897.041ms         6.97%     897.054ms       7.413us        121008  \n",
      "                                            aten::copy_         0.16%      20.969ms        22.50%        2.886s     178.439us     788.119ms         6.13%     806.185ms      49.838us         16176  \n",
      "                                               aten::to         1.98%     254.491ms        22.44%        2.879s     188.589us       0.000us         0.00%      69.524ms       4.555us         15264  \n",
      "                                         aten::_to_copy         0.22%      28.206ms        22.37%        2.870s     215.067us       0.000us         0.00%      75.770ms       5.678us         13344  \n",
      "                                        cudaMemcpyAsync        22.13%        2.839s        22.13%        2.839s      29.568ms      81.000us         0.00%      81.000us       0.844us            96  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.33%      41.929ms        19.85%        2.547s     707.427us       0.000us         0.00%        5.017s       1.394ms          3600  \n",
      "                                   ConvolutionBackward0         0.15%      19.680ms        19.10%        2.451s     680.714us       0.000us         0.00%        4.847s       1.346ms          3600  \n",
      "                             aten::convolution_backward         2.25%     288.713ms        18.96%        2.432s     675.461us        4.157s        32.31%        4.855s       1.349ms          3600  \n",
      "                                              aten::mul         1.29%     165.649ms        11.67%        1.497s      88.257us     318.811ms         2.48%     459.172ms      27.074us         16960  \n",
      "autograd::engine::evaluate_function: NativeGroupNorm...         0.21%      27.134ms         9.29%        1.192s     528.495us       0.000us         0.00%     859.362ms     380.923us          2256  \n",
      "                               NativeGroupNormBackward0         0.13%      17.096ms         8.58%        1.101s     488.111us       0.000us         0.00%     809.680ms     358.901us          2256  \n",
      "                       aten::native_group_norm_backward         0.93%     119.895ms         8.41%        1.079s     478.270us     706.564ms         5.49%     792.216ms     351.160us          2256  \n",
      "    autograd::engine::evaluate_function: RsqrtBackward0         0.08%      10.348ms         7.06%     905.753ms     496.575us       0.000us         0.00%      53.486ms      29.323us          1824  \n",
      "                                         RsqrtBackward0         0.11%      13.980ms         6.98%     895.405ms     490.902us       0.000us         0.00%      53.486ms      29.323us          1824  \n",
      "      autograd::engine::evaluate_function: VarBackward0         0.12%      15.513ms         6.76%     867.149ms     475.411us       0.000us         0.00%     113.197ms      62.060us          1824  \n",
      "                                              aten::sum         1.06%     136.031ms         6.69%     857.739ms      88.903us     350.799ms         2.73%     438.577ms      45.458us          9648  \n",
      "                                           VarBackward0         0.14%      18.593ms         5.75%     737.565ms     404.367us       0.000us         0.00%      99.429ms      54.512us          1824  \n",
      "      autograd::engine::evaluate_function: MulBackward0         0.22%      27.754ms         5.29%     678.725ms     214.244us       0.000us         0.00%     343.623ms     108.467us          3168  \n",
      "                              Optimizer.step#AdamW.step         1.72%     221.205ms         4.14%     531.186ms      11.066ms       0.000us         0.00%      70.949ms       1.478ms            48  \n",
      "                                             aten::add_         0.65%      82.998ms         3.63%     466.164ms      18.427us     498.300ms         3.87%     557.339ms      22.031us         25298  \n",
      "      autograd::engine::evaluate_function: SubBackward0         0.11%      13.717ms         3.52%     452.163ms     247.896us       0.000us         0.00%      35.615ms      19.526us          1824  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...         3.38%     433.354ms         3.38%     433.834ms       8.854ms       0.000us         0.00%       0.000us       0.000us            49  \n",
      "                                           MulBackward0         0.15%      19.403ms         3.25%     416.312ms     131.412us       0.000us         0.00%     245.986ms      77.647us          3168  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.12%      15.045ms         2.43%     311.093ms     308.624us       0.000us         0.00%      98.130ms      97.351us          1008  \n",
      "                                           aten::conv2d         0.10%      12.341ms         2.39%     306.982ms      85.273us       0.000us         0.00%        1.841s     511.394us          3600  \n",
      "                                           SubBackward0         0.03%       3.231ms         2.38%     305.035ms     167.234us       0.000us         0.00%      15.120ms       8.289us          1824  \n",
      "                                              aten::neg         0.11%      14.166ms         2.35%     301.848ms     165.487us       4.084ms         0.03%      15.128ms       8.294us          1824  \n",
      "                                      aten::convolution         0.19%      24.785ms         2.32%     297.271ms      82.575us       0.000us         0.00%        1.856s     515.420us          3600  \n",
      "                                              aten::pow         0.17%      22.295ms         2.31%     296.661ms     162.643us       1.978ms         0.02%      18.183ms       9.969us          1824  \n",
      "                                     aten::_convolution         0.24%      30.787ms         2.12%     272.345ms      75.651us       0.000us         0.00%        1.855s     515.193us          3600  \n",
      "                                        cudaMemsetAsync         1.78%     228.014ms         1.78%     228.014ms      35.987us      34.209ms         0.27%      34.209ms       5.399us          6336  \n",
      "                                              aten::sub         0.36%      46.006ms         1.56%     200.453ms      54.949us      11.800ms         0.09%      38.532ms      10.562us          3648  \n",
      "     autograd::engine::evaluate_function: MeanBackward1         0.12%      15.725ms         1.51%     193.214ms     105.929us       0.000us         0.00%      33.828ms      18.546us          1824  \n",
      "                                aten::cudnn_convolution         1.07%     137.581ms         1.48%     189.643ms      52.679us        1.406s        10.93%        1.560s     433.345us          3600  \n",
      "                                              aten::bmm         0.34%      43.852ms         1.36%     174.513ms      67.328us        2.847s        22.13%        2.865s       1.105ms          2592  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.05%       6.461ms         1.33%     170.907ms     197.809us       0.000us         0.00%        1.717s       1.988ms           864  \n",
      "                                         AddmmBackward0         0.08%      10.170ms         1.33%     170.285ms     168.934us       0.000us         0.00%      61.994ms      61.502us          1008  \n",
      "                                       aten::group_norm         0.05%       6.506ms         1.30%     166.770ms      73.923us       0.000us         0.00%     457.761ms     202.908us          2256  \n",
      "                                           BmmBackward0         0.06%       7.101ms         1.28%     163.992ms     189.806us       0.000us         0.00%        1.695s       1.962ms           864  \n",
      "                                aten::native_group_norm         0.58%      74.854ms         1.25%     160.264ms      71.039us     419.709ms         3.26%     457.761ms     202.908us          2256  \n",
      "                                            aten::empty         1.21%     155.193ms         1.21%     155.193ms       4.001us       0.000us         0.00%       0.000us       0.000us         38785  \n",
      "                                             aten::mean         0.41%      52.924ms         1.18%     150.798ms      40.800us      52.370ms         0.41%      93.103ms      25.190us          3696  \n",
      "                                               aten::mm         0.29%      37.404ms         1.15%     146.910ms      74.649us      22.963ms         0.18%      61.994ms      31.501us          1968  \n",
      "                                                INVALID         0.99%     127.334ms         0.99%     127.334ms      27.633us      28.772ms         0.22%      31.355ms       6.804us          4608  \n",
      "                                              aten::add         0.49%      62.286ms         0.96%     123.176ms      22.510us     288.686ms         2.24%     322.401ms      58.918us          5472  \n",
      "      autograd::engine::evaluate_function: AddBackward0         0.20%      26.120ms         0.94%     120.976ms      24.234us       0.000us         0.00%     104.906ms      21.015us          4992  \n",
      "                                          aten::reshape         0.31%      39.654ms         0.88%     113.048ms       8.595us       0.000us         0.00%     653.851ms      49.715us         13152  \n",
      "                                    aten::_foreach_add_         0.20%      25.956ms         0.74%      95.350ms     993.229us       6.661ms         0.05%       7.247ms      75.490us            96  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.34%      43.742ms         0.74%      94.986ms       7.144us       0.000us         0.00%       0.000us       0.000us         13296  \n",
      "                                    aten::empty_strided         0.74%      94.515ms         0.74%      94.515ms       2.358us       0.000us         0.00%       0.000us       0.000us         40080  \n",
      "                                          MeanBackward1         0.07%       8.875ms         0.73%      93.881ms      51.470us       0.000us         0.00%      14.917ms       8.178us          1824  \n",
      "                                           aten::matmul         0.07%       8.598ms         0.63%      81.345ms      94.149us       0.000us         0.00%        1.490s       1.725ms           864  \n",
      "                                            aten::clone         0.09%      11.739ms         0.60%      77.548ms      27.383us       0.000us         0.00%     724.739ms     255.911us          2832  \n",
      "                                              aten::div         0.20%      25.930ms         0.60%      76.354ms      41.384us       5.444ms         0.04%      15.912ms       8.624us          1845  \n",
      "                                     aten::_foreach_add         0.23%      29.131ms         0.59%      76.058ms       1.585ms       5.717ms         0.04%       7.028ms     146.417us            48  \n",
      "                                    aten::_foreach_sqrt         0.16%      20.334ms         0.55%      70.756ms       1.474ms       6.143ms         0.05%       8.807ms     183.479us            48  \n",
      "    autograd::engine::evaluate_function: SplitBackward0         0.06%       8.065ms         0.45%      57.487ms      42.773us       0.000us         0.00%     380.280ms     282.946us          1344  \n",
      "                                              aten::cat         0.19%      23.981ms         0.44%      55.912ms      30.654us     463.968ms         3.61%     478.282ms     262.216us          1824  \n",
      "                                           aten::linear         0.03%       4.207ms         0.40%      51.479ms      51.070us       0.000us         0.00%      27.872ms      27.651us          1008  \n",
      "                                         SplitBackward0         0.03%       3.889ms         0.38%      49.343ms      36.714us       0.000us         0.00%     380.257ms     282.929us          1344  \n",
      "                        torch::autograd::AccumulateGrad         0.17%      22.415ms         0.37%      46.829ms       3.522us       0.000us         0.00%       0.000us       0.000us         13296  \n",
      "                                              aten::var         0.19%      24.561ms         0.36%      46.090ms      25.269us      33.055ms         0.26%      54.223ms      29.728us          1824  \n",
      "                                            aten::addmm         0.23%      29.031ms         0.32%      40.962ms      40.637us      15.069ms         0.12%      28.191ms      27.967us          1008  \n",
      "autograd::engine::evaluate_function: ReshapeAliasBac...         0.06%       7.777ms         0.29%      37.728ms      13.552us       0.000us         0.00%     186.725ms      67.071us          2784  \n",
      "     autograd::engine::evaluate_function: SiluBackward0         0.05%       6.191ms         0.28%      36.531ms      40.056us       0.000us         0.00%     110.199ms     120.832us           912  \n",
      "                                    aten::_foreach_mul_         0.22%      28.529ms         0.24%      31.049ms     215.618us      12.021ms         0.09%      20.831ms     144.660us           144  \n",
      "                                            aten::rsqrt         0.16%      20.323ms         0.24%      30.299ms      16.611us       2.672ms         0.02%      18.311ms      10.039us          1824  \n",
      "                                          SiluBackward0         0.02%       3.202ms         0.23%      29.219ms      32.038us       0.000us         0.00%     105.436ms     115.610us           912  \n",
      "                                  ReshapeAliasBackward0         0.04%       5.669ms         0.22%      27.783ms       9.980us       0.000us         0.00%     169.866ms      61.015us          2784  \n",
      "                                           aten::detach         0.10%      13.423ms         0.21%      27.492ms       2.068us       0.000us         0.00%       0.000us       0.000us         13296  \n",
      "                                    aten::silu_backward         0.09%      11.440ms         0.21%      27.138ms      29.757us     105.103ms         0.82%     110.199ms     120.832us           912  \n",
      "                                                aten::t         0.09%      11.289ms         0.19%      24.286ms       4.865us       0.000us         0.00%       0.000us       0.000us          4992  \n",
      "                                        aten::transpose         0.15%      19.444ms         0.18%      23.691ms       2.804us       0.000us         0.00%       0.000us       0.000us          8448  \n",
      "                    Optimizer.zero_grad#AdamW.zero_grad         0.17%      22.245ms         0.17%      22.245ms     463.438us       0.000us         0.00%       0.000us       0.000us            48  \n",
      "autograd::engine::evaluate_function: UnsafeViewBackw...         0.06%       7.844ms         0.17%      22.155ms       8.547us       0.000us         0.00%       0.000us       0.000us          2592  \n",
      "                                       aten::contiguous         0.01%     834.000us         0.17%      21.876ms      35.058us       0.000us         0.00%      65.368ms     104.756us           624  \n",
      "                                       aten::empty_like         0.05%       5.783ms         0.17%      21.783ms       7.440us       0.000us         0.00%       0.000us       0.000us          2928  \n",
      "                                            aten::chunk         0.03%       3.588ms         0.17%      21.524ms      16.015us       0.000us         0.00%       0.000us       0.000us          1344  \n",
      "                                            aten::split         0.06%       7.610ms         0.15%      19.420ms      14.449us       0.000us         0.00%       0.000us       0.000us          1344  \n",
      "autograd::engine::evaluate_function: SoftmaxBackward...         0.02%       2.037ms         0.15%      19.395ms      44.896us       0.000us         0.00%      31.850ms      73.727us           432  \n",
      "                                       SoftmaxBackward0         0.01%       1.889ms         0.13%      17.108ms      39.602us       0.000us         0.00%      31.356ms      72.583us           432  \n",
      "                                           aten::expand         0.10%      12.613ms         0.12%      15.753ms       4.435us       0.000us         0.00%       0.000us       0.000us          3552  \n",
      "                                           aten::narrow         0.05%       6.481ms         0.12%      15.644ms       3.927us       0.000us         0.00%       0.000us       0.000us          3984  \n",
      "                           aten::_softmax_backward_data         0.04%       5.419ms         0.12%      15.469ms      35.808us      16.462ms         0.13%      31.850ms      73.727us           432  \n",
      "                                                 detach         0.12%      15.406ms         0.12%      15.406ms       1.159us       0.000us         0.00%       0.000us       0.000us         13296  \n",
      "                                   aten::_reshape_alias         0.12%      15.351ms         0.12%      15.351ms       1.397us       0.000us         0.00%       0.000us       0.000us         10992  \n",
      "                                             aten::view         0.11%      14.639ms         0.11%      14.639ms       0.523us       0.000us         0.00%       0.000us       0.000us         27987  \n",
      "                                    UnsafeViewBackward0         0.04%       4.581ms         0.10%      13.285ms       5.125us       0.000us         0.00%       0.000us       0.000us          2592  \n",
      "                                          aten::resize_         0.10%      13.272ms         0.10%      13.272ms       3.215us       0.000us         0.00%       0.000us       0.000us          4128  \n",
      "                                             aten::silu         0.07%       9.050ms         0.10%      13.077ms      14.339us      69.167ms         0.54%      83.367ms      91.411us           912  \n",
      "                                       aten::as_strided         0.10%      12.864ms         0.10%      12.864ms       0.663us       0.000us         0.00%       0.000us       0.000us         19392  \n",
      "                                aten::_foreach_addcdiv_         0.09%      11.289ms         0.10%      12.454ms     259.458us       9.458ms         0.07%      12.930ms     269.375us            48  \n",
      "                                aten::_foreach_addcmul_         0.08%      10.812ms         0.09%      11.944ms     248.833us       7.790ms         0.06%       8.313ms     173.188us            48  \n",
      "                                    aten::_foreach_div_         0.07%       9.506ms         0.08%      10.229ms     213.104us       5.398ms         0.04%       5.565ms     115.938us            48  \n",
      "                                            aten::slice         0.06%       8.028ms         0.08%      10.042ms       2.521us       0.000us         0.00%       0.000us       0.000us          3984  \n",
      "        autograd::engine::evaluate_function: TBackward0         0.03%       3.323ms         0.07%       8.610ms       8.542us       0.000us         0.00%       0.000us       0.000us          1008  \n",
      "      autograd::engine::evaluate_function: CatBackward0         0.01%       1.888ms         0.06%       8.258ms      19.116us       0.000us         0.00%       0.000us       0.000us           432  \n",
      "                                          aten::softmax         0.01%     661.000us         0.06%       7.520ms      17.407us       0.000us         0.00%      14.842ms      34.356us           432  \n",
      "autograd::engine::evaluate_function: TransposeBackwa...         0.02%       2.778ms         0.05%       7.023ms       8.128us       0.000us         0.00%       0.000us       0.000us           864  \n",
      "                                         aten::_softmax         0.04%       4.933ms         0.05%       6.911ms      15.998us      12.638ms         0.10%      14.965ms      34.641us           432  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 12.828s\n",
      "Self CUDA time total: 12.866s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by='cpu_time_total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3ad55-a493-4c93-b1d9-dc6514af7f45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Optimization 2 - `torch.compile`\n",
    "\n",
    "8.3x Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a319793e-5601-4d5e-92d0-545ac78ae52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "img2tensor = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=0.5, std=0.5)  # [0, 1] -> [-1, 1]\n",
    "])\n",
    "ds = CIFAR10('./cifar10', train=True, transform=img2tensor, download=True)\n",
    "cfg_m = ModelConfig()\n",
    "cfg_t = TrainerConfig(bs=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8fc285-0f28-4396-92b3-1975d8e55aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ds, batch_size=cfg_t.bs, num_workers=cfg_t.num_workers, drop_last=True)\n",
    "ddpm = torch.compile(DDPM(**asdict(cfg_m)).to('cuda'))\n",
    "optimizer = torch.optim.AdamW(ddpm.parameters(), lr=cfg_t.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b56f2b4-125d-4c70-b3e5-770ec0da1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, _ = next(iter(dataloader))\n",
    "x0 = x0.to('cuda')\n",
    "eps = torch.randn_like(x0)\n",
    "t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e386fbff-6cef-4fc1-a2d4-1a352d8f4feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "eps_pred = ddpm(x0, eps, t)\n",
    "loss = F.smooth_l1_loss(eps, eps_pred)    \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8298eea8-6e9e-4568-a0c0-db5a8623c479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.651767578125\n"
     ]
    }
   ],
   "source": [
    "ddpm.train()\n",
    "\n",
    "start_e = torch.cuda.Event(enable_timing=True)\n",
    "end_e = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_e.record()\n",
    "\n",
    "for _, (x0, _) in zip(range(100), dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x0 = x0.to('cuda')\n",
    "    eps = torch.randn_like(x0)\n",
    "    t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "    \n",
    "    eps_pred = ddpm(x0, eps, t)\n",
    "    loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "end_e.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "t = start_e.elapsed_time(end_e) / 100.0\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a8e638-8540-4a32-96ef-f59fde08dc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.154026353237875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflops_per_iter = (489.73 * 2 * 3) / 1000\n",
    "tflop_per_sec = tflops_per_iter / (t / 1000)\n",
    "tflop_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c7d8766-cb76-406b-a991-111e5762712c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.300352698707172"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflop_per_sec / BASELINE_TFLOPS_PER_SEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a095e-50e1-45b6-932d-61d3fe62e37c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Optimization 3 - Use PyTorch SDPA\n",
    "\n",
    "- [PyTorch SDPA doc](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n",
    "- [NanoGPT Usage](https://github.com/karpathy/nanoGPT/blob/master/model.py)\n",
    "\n",
    "8.64x speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e098929-c8b3-42d1-855d-06043b0092e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "img2tensor = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=0.5, std=0.5)  # [0, 1] -> [-1, 1]\n",
    "])\n",
    "ds = CIFAR10('./cifar10', train=True, transform=img2tensor, download=True)\n",
    "cfg_m = ModelConfig()\n",
    "cfg_t = TrainerConfig(bs=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66105797-b690-42e4-9f55-f58a1b74d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ds, batch_size=cfg_t.bs, num_workers=cfg_t.num_workers, drop_last=True)\n",
    "ddpm = torch.compile(DDPM(**asdict(cfg_m)).to('cuda'))\n",
    "optimizer = torch.optim.AdamW(ddpm.parameters(), lr=cfg_t.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4199fea5-ea51-4d31-8f60-477aede36ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x0, _ = next(iter(dataloader))\n",
    "x0 = x0.to('cuda')\n",
    "eps = torch.randn_like(x0)\n",
    "t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "\n",
    "eps_pred = ddpm(x0, eps, t)\n",
    "loss = F.smooth_l1_loss(eps, eps_pred)    \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f813de-4c4f-4524-a8a1-c31e4492c9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.928603515625\n"
     ]
    }
   ],
   "source": [
    "ddpm.train()\n",
    "\n",
    "start_e = torch.cuda.Event(enable_timing=True)\n",
    "end_e = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_e.record()\n",
    "\n",
    "for _, (x0, _) in zip(range(100), dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x0 = x0.to('cuda')\n",
    "    eps = torch.randn_like(x0)\n",
    "    t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "    \n",
    "    eps_pred = ddpm(x0, eps, t)\n",
    "    loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "end_e.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "t = start_e.elapsed_time(end_e) / 100.0\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cccb5c33-f6ef-45ec-8451-d53ada7dbbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.129693775974573"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflops_per_iter = (489.73 * 2 * 3) / 1000\n",
    "tflop_per_sec = tflops_per_iter / (t / 1000)\n",
    "tflop_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfb4dec4-7c2b-4418-8ebd-cd43792a13fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.635633599991262"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflop_per_sec / BASELINE_TFLOPS_PER_SEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a58218-6534-4219-8c16-bda41ac025fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Optimization 3.5 - Use TF32 Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db55940-0552-4b61-8f53-ce3d78d85c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7503ac4d-9e45-4edc-9939-9142a30c4f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "img2tensor = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=0.5, std=0.5)  # [0, 1] -> [-1, 1]\n",
    "])\n",
    "ds = CIFAR10('./cifar10', train=True, transform=img2tensor, download=True)\n",
    "cfg_m = ModelConfig()\n",
    "cfg_t = TrainerConfig(bs=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33c8c642-1fe7-4567-828f-ffa5a4240d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ds, batch_size=cfg_t.bs, num_workers=cfg_t.num_workers, drop_last=True)\n",
    "ddpm = torch.compile(DDPM(**asdict(cfg_m)).to('cuda'))\n",
    "optimizer = torch.optim.AdamW(ddpm.parameters(), lr=cfg_t.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60ea3e3-0651-4777-b474-3f5a5efd0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, _ = next(iter(dataloader))\n",
    "x0 = x0.to('cuda')\n",
    "eps = torch.randn_like(x0)\n",
    "t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "\n",
    "eps_pred = ddpm(x0, eps, t)\n",
    "loss = F.smooth_l1_loss(eps, eps_pred)    \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "480f9dca-968c-4afc-a54d-1a1e812b042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.965390625\n"
     ]
    }
   ],
   "source": [
    "ddpm.train()\n",
    "\n",
    "start_e = torch.cuda.Event(enable_timing=True)\n",
    "end_e = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_e.record()\n",
    "\n",
    "for _, (x0, _) in zip(range(100), dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x0 = x0.to('cuda')\n",
    "    eps = torch.randn_like(x0)\n",
    "    t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "    \n",
    "    eps_pred = ddpm(x0, eps, t)\n",
    "    loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "end_e.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "t = start_e.elapsed_time(end_e) / 100.0\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fbb0d88-7d49-4dc1-b6da-35e751697095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.121790166295185"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflops_per_iter = (489.73 * 2 * 3) / 1000\n",
    "tflop_per_sec = tflops_per_iter / (t / 1000)\n",
    "tflop_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e6cec2e-f591-4632-90a2-2f960202c2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.632917582919307"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflop_per_sec / BASELINE_TFLOPS_PER_SEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082e076-2569-4dd4-89ca-4ed19f3f0c9e",
   "metadata": {},
   "source": [
    "## Optimization 4 - Mixed-Precision Training\n",
    "\n",
    "Lowering to 16-bit precision lowers the memory requirements, so we further increase the batch size.  \n",
    "At batch size 1024, it achieves 10.99x speedup.  \n",
    "At batch size 3584, it achieves 11.92x speedup.\n",
    "\n",
    "- [Tutorial 1](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/)\n",
    "- [Tutorial 2](https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2a6663-7cd0-40c0-8334-d93384c05448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "img2tensor = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=0.5, std=0.5)  # [0, 1] -> [-1, 1]\n",
    "])\n",
    "ds = CIFAR10('./cifar10', train=True, transform=img2tensor, download=True)\n",
    "cfg_m = ModelConfig()\n",
    "cfg_t = TrainerConfig(bs=3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b653c70-4508-4117-8455-a3b1a9f56196",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ds, batch_size=cfg_t.bs, num_workers=cfg_t.num_workers, drop_last=True)\n",
    "ddpm = torch.compile(DDPM(**asdict(cfg_m)).to('cuda'))\n",
    "optimizer = torch.optim.AdamW(ddpm.parameters(), lr=cfg_t.lr)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67e46a8-3c0a-4ad7-afeb-f8bd60a21002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "DDPM                                          [3072, 3, 32, 32]         --\n",
       "├─UNet: 1-1                                   [3072, 3, 32, 32]         --\n",
       "│    └─Sequential: 2-1                        [3072, 128]               --\n",
       "│    │    └─Linear: 3-1                       [3072, 128]               4,224\n",
       "│    │    └─GELU: 3-2                         [3072, 128]               --\n",
       "│    │    └─Linear: 3-3                       [3072, 128]               16,512\n",
       "│    └─Conv2d: 2-2                            [3072, 32, 32, 32]        128\n",
       "│    └─UNetDownsample: 2-3                    [3072, 32, 16, 16]        --\n",
       "│    │    └─TimeResNetBlock: 3-4              [3072, 32, 32, 32]        26,880\n",
       "│    │    └─TimeResNetBlock: 3-5              [3072, 32, 32, 32]        26,880\n",
       "│    │    └─GroupNorm: 3-6                    [3072, 32, 32, 32]        64\n",
       "│    │    └─Attention: 3-7                    [3072, 32, 32, 32]        16,416\n",
       "│    │    └─DownsampleOutProject: 3-8         [3072, 32, 16, 16]        4,128\n",
       "│    └─UNetDownsample: 2-4                    [3072, 64, 8, 8]          --\n",
       "│    │    └─TimeResNetBlock: 3-9              [3072, 32, 16, 16]        26,880\n",
       "│    │    └─TimeResNetBlock: 3-10             [3072, 32, 16, 16]        26,880\n",
       "│    │    └─GroupNorm: 3-11                   [3072, 32, 16, 16]        64\n",
       "│    │    └─Attention: 3-12                   [3072, 32, 16, 16]        16,416\n",
       "│    │    └─DownsampleOutProject: 3-13        [3072, 64, 8, 8]          8,256\n",
       "│    └─UNetDownsample: 2-5                    [3072, 128, 4, 4]         --\n",
       "│    │    └─TimeResNetBlock: 3-14             [3072, 64, 8, 8]          90,624\n",
       "│    │    └─TimeResNetBlock: 3-15             [3072, 64, 8, 8]          90,624\n",
       "│    │    └─GroupNorm: 3-16                   [3072, 64, 8, 8]          128\n",
       "│    │    └─Attention: 3-17                   [3072, 64, 8, 8]          32,832\n",
       "│    │    └─DownsampleOutProject: 3-18        [3072, 128, 4, 4]         32,896\n",
       "│    └─UNetDownsample: 2-6                    [3072, 256, 4, 4]         --\n",
       "│    │    └─TimeResNetBlock: 3-19             [3072, 128, 4, 4]         328,704\n",
       "│    │    └─TimeResNetBlock: 3-20             [3072, 128, 4, 4]         328,704\n",
       "│    │    └─GroupNorm: 3-21                   [3072, 128, 4, 4]         256\n",
       "│    │    └─Attention: 3-22                   [3072, 128, 4, 4]         65,664\n",
       "│    │    └─Conv2d: 3-23                      [3072, 256, 4, 4]         295,168\n",
       "│    └─UNetBlock: 2-7                         [3072, 256, 4, 4]         --\n",
       "│    │    └─TimeResNetBlock: 3-24             [3072, 256, 4, 4]         1,247,232\n",
       "│    │    └─TimeResNetBlock: 3-25             [3072, 256, 4, 4]         1,247,232\n",
       "│    │    └─GroupNorm: 3-26                   [3072, 256, 4, 4]         512\n",
       "│    │    └─Attention: 3-27                   [3072, 256, 4, 4]         131,328\n",
       "│    └─UNetUpsample: 2-8                      [3072, 128, 8, 8]         --\n",
       "│    │    └─TimeResNetBlock: 3-28             [3072, 256, 4, 4]         1,640,704\n",
       "│    │    └─TimeResNetBlock: 3-29             [3072, 256, 4, 4]         1,640,704\n",
       "│    │    └─GroupNorm: 3-30                   [3072, 256, 4, 4]         512\n",
       "│    │    └─Attention: 3-31                   [3072, 256, 4, 4]         131,328\n",
       "│    │    └─Sequential: 3-32                  [3072, 128, 8, 8]         295,040\n",
       "│    └─UNetUpsample: 2-9                      [3072, 64, 16, 16]        --\n",
       "│    │    └─TimeResNetBlock: 3-33             [3072, 128, 8, 8]         427,136\n",
       "│    │    └─TimeResNetBlock: 3-34             [3072, 128, 8, 8]         427,136\n",
       "│    │    └─GroupNorm: 3-35                   [3072, 128, 8, 8]         256\n",
       "│    │    └─Attention: 3-36                   [3072, 128, 8, 8]         65,664\n",
       "│    │    └─Sequential: 3-37                  [3072, 64, 16, 16]        73,792\n",
       "│    └─UNetUpsample: 2-10                     [3072, 32, 32, 32]        --\n",
       "│    │    └─TimeResNetBlock: 3-38             [3072, 64, 16, 16]        115,264\n",
       "│    │    └─TimeResNetBlock: 3-39             [3072, 64, 16, 16]        115,264\n",
       "│    │    └─GroupNorm: 3-40                   [3072, 64, 16, 16]        128\n",
       "│    │    └─Attention: 3-41                   [3072, 64, 16, 16]        32,832\n",
       "│    │    └─Sequential: 3-42                  [3072, 32, 32, 32]        18,464\n",
       "│    └─UNetUpsample: 2-11                     [3072, 32, 32, 32]        --\n",
       "│    │    └─TimeResNetBlock: 3-43             [3072, 32, 32, 32]        38,176\n",
       "│    │    └─TimeResNetBlock: 3-44             [3072, 32, 32, 32]        38,176\n",
       "│    │    └─GroupNorm: 3-45                   [3072, 32, 32, 32]        64\n",
       "│    │    └─Attention: 3-46                   [3072, 32, 32, 32]        16,416\n",
       "│    │    └─Conv2d: 3-47                      [3072, 32, 32, 32]        9,248\n",
       "│    └─TimeResNetBlock: 2-12                  [3072, 32, 32, 32]        --\n",
       "│    │    └─Linear: 3-48                      [3072, 64]                8,256\n",
       "│    │    └─Sequential: 3-49                  [3072, 32, 32, 32]        18,528\n",
       "│    │    └─Sequential: 3-50                  [3072, 32, 32, 32]        9,312\n",
       "│    │    └─Conv2d: 3-51                      [3072, 32, 32, 32]        2,080\n",
       "│    └─Conv2d: 2-13                           [3072, 3, 32, 32]         99\n",
       "===============================================================================================\n",
       "Total params: 9,190,211\n",
       "Trainable params: 9,190,211\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (T): 1.47\n",
       "===============================================================================================\n",
       "Input size (MB): 75.52\n",
       "Forward/backward pass size (MB): 64202.74\n",
       "Params size (MB): 36.76\n",
       "Estimated Total Size (MB): 64315.02\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "x0, _ = next(iter(dataloader))\n",
    "x0 = x0.to('cuda')\n",
    "eps = torch.randn_like(x0)\n",
    "t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "summary(DDPM(**asdict(cfg_m)).to('cuda'), input_data=[x0, eps, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0857df3-cf82-48f9-827c-4d61706e021a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.82"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflops_per_iter = 1.47 * 2 * 3\n",
    "tflops_per_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c695cda-081d-412c-8cc1-cacf7d0f7dc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0, _ = next(iter(dataloader))\n",
    "x0 = x0.to('cuda')\n",
    "eps = torch.randn_like(x0)\n",
    "t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    eps_pred = ddpm(x0, eps, t)\n",
    "\n",
    "loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "scaler.scale(loss).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d47d1a97-bcc2-4da1-bbc2-dd9d692ac733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ddpm.train()\n",
    "\n",
    "n_steps = 16\n",
    "starts = [torch.cuda.Event(enable_timing=True) for _ in range(n_steps)]\n",
    "ends = [torch.cuda.Event(enable_timing=True) for _ in range(n_steps)]\n",
    "\n",
    "for i, (x0, _) in zip(range(n_steps), dataloader):\n",
    "    starts[i].record()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x0 = x0.to('cuda')\n",
    "    eps = torch.randn_like(x0)\n",
    "    t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "        eps_pred = ddpm(x0, eps, t)\n",
    "    loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    ends[i].record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "t = np.mean([s.elapsed_time(e) for s, e in zip(starts, ends)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1218252e-cd31-4724-a100-9bcf0cc34363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[479.6488037109375,\n",
       " 460.52862548828125,\n",
       " 460.4241943359375,\n",
       " 461.0775146484375,\n",
       " 460.2716064453125,\n",
       " 460.305419921875,\n",
       " 460.3299865722656,\n",
       " 460.33099365234375,\n",
       " 460.2429504394531,\n",
       " 460.2245178222656,\n",
       " 460.9413146972656,\n",
       " 460.6423034667969,\n",
       " 460.0217590332031,\n",
       " 460.24090576171875,\n",
       " 460.5327453613281,\n",
       " 460.4999694824219]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.elapsed_time(e) for s, e in zip(starts, ends)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78a00cff-dc61-4e6f-abb5-411839c19c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.327724609375\n"
     ]
    }
   ],
   "source": [
    "ddpm.train()\n",
    "\n",
    "start_e = torch.cuda.Event(enable_timing=True)\n",
    "end_e = torch.cuda.Event(enable_timing=True)\n",
    "start_e.record()\n",
    "\n",
    "for _, (x0, _) in zip(range(100), dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x0 = x0.to('cuda')\n",
    "    eps = torch.randn_like(x0)\n",
    "    t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "        eps_pred = ddpm(x0, eps, t)\n",
    "    loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "end_e.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "t = start_e.elapsed_time(end_e) / 100.0\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b97d7ff-c62a-4080-8491-01bd67eb1c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.105735651364622"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflop_per_sec = tflops_per_iter / (t / 1000)\n",
    "tflop_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe180278-8e70-4159-8b49-91a2219c632b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.565544897376158"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflop_per_sec / BASELINE_TFLOPS_PER_SEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc7ca6-525d-4cee-8453-57c2d586e1ea",
   "metadata": {},
   "source": [
    "## Optimization 5 - Faster Data Loading FFCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c90324dc-7260-439b-89b2-0e21a6955214",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                               | 0/50000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 124044.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import RGBImageField\n",
    "\n",
    "ds = CIFAR10('./cifar10', train=True, download=True)\n",
    "writer = DatasetWriter('./cifar10.beton', {'image': RGBImageField(max_resolution=32)})\n",
    "writer.from_indexed_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956e11f-2b13-4403-87f6-de367e1f888b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.fields.decoders import SimpleRGBImageDecoder\n",
    "from ffcv.transforms import ToTensor, ToTorchImage, ToDevice, NormalizeImage, RandomHorizontalFlip, Convert\n",
    "\n",
    "img_tsfms = [\n",
    "    SimpleRGBImageDecoder(),\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    ToDevice(torch.device('cuda')),\n",
    "    ToTorchImage(),\n",
    "    NormalizeImage(\n",
    "        mean=np.array([127.5, 127.5, 127.5]), std=np.array([127.5, 127.5, 127.5]),  # [0, 255] -> [-1, 1]\n",
    "        type=np.float32\n",
    "    )\n",
    "]\n",
    "dataloader = Loader(\n",
    "    './cifar10.beton', batch_size=1, num_workers=4, # os_cache=True,\n",
    "    order=OrderOption.RANDOM, pipelines={'image': img_tsfms}\n",
    ")\n",
    "x0 = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bf59a09-01f5-4b73-8783-83bf3862783a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32,\n",
       " torch.Size([1, 3, 32, 32]),\n",
       " tensor(-0.8824, device='cuda:0'),\n",
       " tensor(0.8980, device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0[0].dtype, x0[0].size(), x0[0].min(), x0[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ca8aa-6d85-48c1-8172-cd0ffbb157c9",
   "metadata": {},
   "source": [
    "### Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e4cad5-d24e-4d64-bbaf-a862d66bfca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_m = ModelConfig()\n",
    "cfg_t = TrainerConfig(bs=3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cd87d-c532-4ccd-acc7-b49c6869ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.fields.decoders import SimpleRGBImageDecoder\n",
    "from ffcv.transforms import ToTensor, ToTorchImage, ToDevice, NormalizeImage, RandomHorizontalFlip, Convert\n",
    "\n",
    "img_tsfms = [\n",
    "    SimpleRGBImageDecoder(),\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    ToDevice(torch.device(cfg_t.device)),\n",
    "    ToTorchImage(),\n",
    "    NormalizeImage(\n",
    "        mean=np.array([127.5, 127.5, 127.5]), std=np.array([127.5, 127.5, 127.5]),  # [0, 255] -> [-1, 1]\n",
    "        type=np.float32\n",
    "    )\n",
    "]\n",
    "dataloader = Loader(\n",
    "    './cifar10.beton', batch_size=cfg_t.bs, num_workers=cfg_t.num_workers, drop_last=True, os_cache=True,\n",
    "    order=OrderOption.RANDOM, pipelines={'image': img_tsfms}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3963f2a6-d572-4dcf-b062-17fcf62dbb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm = torch.compile(DDPM(**asdict(cfg_m)).to('cuda'))\n",
    "optimizer = torch.optim.AdamW(ddpm.parameters(), lr=cfg_t.lr)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25426099-b578-4d4d-84c3-89ede4f97954",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, = next(iter(dataloader))\n",
    "# x0 = x0.to('cuda')\n",
    "eps = torch.randn_like(x0)\n",
    "t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    eps_pred = ddpm(x0, eps, t)\n",
    "    loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "scaler.scale(loss).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "808d0b48-0f05-4c82-a446-65fa51554b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.9346142578125\n"
     ]
    }
   ],
   "source": [
    "ddpm.train()\n",
    "\n",
    "start_e = torch.cuda.Event(enable_timing=True)\n",
    "end_e = torch.cuda.Event(enable_timing=True)\n",
    "start_e.record()\n",
    "\n",
    "for _, (x0, ) in zip(range(100), dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # x0 = x0.to('cuda')\n",
    "    eps = torch.randn_like(x0)\n",
    "    t = torch.randint(0, cfg_m.nT, [cfg_t.bs], device=x0.device)\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        eps_pred = ddpm(x0, eps, t)\n",
    "        loss = F.smooth_l1_loss(eps, eps_pred)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "end_e.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "t = start_e.elapsed_time(end_e) / 100.0\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87a0d9c6-b1e5-4813-be20-f53d604e18e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.28786646643916"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflops_per_iter = (489.73 * 2 * 3) / 1000\n",
    "tflop_per_sec = tflops_per_iter / (t / 1000)\n",
    "tflop_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4adc58cf-4837-49b1-8c15-ec812a65ff72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.844627651697305"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflop_per_sec / BASELINE_TFLOPS_PER_SEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb190cb-e6f9-40be-9a2b-60ffc41c12f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
